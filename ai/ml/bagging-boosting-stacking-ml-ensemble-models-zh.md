# 机器学习中的装袋、提升和堆叠

[机器学习](https://www.baeldung.com/cs/category/ai/ml)

[训练](https://www.baeldung.com/cs/tag/training)

1. 简介

    装袋、提升和堆叠属于一类机器学习算法，被称为[集合学习算法](https://www.baeldung.com/cs/ensemble-learning#definition-of-ensemble-learning)。 集合学习是将多个模型的预测结果合而为一，以提高预测性能。

    在本教程中，我们将回顾一下装袋、提升和堆叠之间的区别。

2. 装袋

    袋式学习也称为[引导](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))聚合，是一种集合学习技术，它结合了引导和聚合的优点，可以产生一个稳定的模型，提高机器学习模型的预测性能。

    在bagging中，我们首先通过引导从数据集中抽取等大小的数据子集，即进行替换抽样。然后，我们使用这些子集来独立训练多个弱模型。弱模型就是预测准确率低的模型。相反，强模型则非常准确。为了得到一个强模型，我们将所有弱模型的预测结果进行汇总：

    bagging的步骤显示，训练数据被分成三个不同的子集，并在三个不同的模型上并行训练。

    ![bagging的步骤](pic/img_64bd9575e6fc8.svg)

    因此，共有三个步骤：

    - 对大小相等的子集进行替换采样
    - 在每个子集上独立并行地训练弱模型
    - 通过求平均值或投票的方式合并每个弱模型的结果，得到最终结果

    在[回归](https://www.baeldung.com/cs/correlation-coefficient-vs-regression-model#regression)任务中，通过求平均值来汇总结果；在[分类](https://www.baeldung.com/cs/ml-classification-vs-clustering#1-introduction-to-classification)任务中，通过选取多数类来汇总结果。

    1. 使用袋装法的算法

        套袋算法的主要思想是减少数据集中的方差，确保模型的鲁棒性，不受数据集中特定样本的影响。

        因此，袋式分类主要应用于[决策树](https://www.baeldung.com/cs/decision-trees-vs-random-forests#decision-trees)和[随机森林](https://www.baeldung.com/cs/decision-trees-vs-random-forests#random-forests)等基于树的机器学习模型。

    2. 袋式算法的优缺点

        下面是对套袋法的快速总结：

        | 优点            | 缺点                 |
        |---------------|--------------------|
        | 提高整体准确性       | 并行训练许多弱模型的计算成本可能很高 |
        | 减少总体差异        | 大量弱模型可能会降低模型的可解释性  |
        | 增强模型对数据噪声的稳健性 |                    |

3. 提升法

    在提升法中，我们训练一系列模型。每个模型都在一个加权训练集上训练。我们根据序列中前一个模型的误差来分配权重。

    序列训练背后的主要理念是让每个模型纠正前一个模型的错误。这种训练一直持续到训练完预定数量的模型或满足其他标准为止。

    在训练过程中，被错误分类的实例会被赋予更高的权重，以便在使用下一个模型训练时获得某种形式的优先权：

    ![boosting步骤](pic/img_64bd9576a9485.svg)

    提升的步骤显示了依次用三个不同模型训练的加权训练数据。
    此外，在将弱模型的预测结果合并为最终输出结果时，弱模型的权重会低于强模型。

    因此，我们首先将数据权重初始化为相同的值，然后反复执行以下步骤：

    - 在所有实例上训练模型
    - 计算所有实例的模型输出误差
    - 为模型分配权重（权重高则性能好，反之亦然）
    - 更新数据权重：给误差大的样本更高的权重
    - 如果性能不理想或满足其他停止条件，重复前面的步骤

    最后，我们将这些模型合并成一个用于预测的模型。

    1. 使用提升的算法

        助推通常是通过提高弱学习者的性能来提高机器学习模型的准确性。我们通常使用 [XGBoost](https://arxiv.org/abs/1603.02754)、[CatBoost](https://arxiv.org/abs/1706.09516) 和 [AdaBoost](https://www.matec-conferences.org/articles/matecconf/abs/2017/53/matecconf_icmite2017_00222/matecconf_icmite2017_00222.html)。

        这些算法应用了不同的提升技术，最著名的是它们都能实现出色的性能。

    2. 提升的利与弊

        提升算法有很多优点，但也不是没有缺点：

        | 优点                 | 缺点            |
        |--------------------|---------------|
        | 提高整体准确性            | 计算成本可能很高      |
        | 通过改进先前模型的弱点，减少整体偏差 | 对噪声数据敏感       |
        |                    | 模型依赖性可能导致错误复制 |

        是否使用提升法取决于数据是否没有噪声以及我们的计算能力。

4. 堆叠

    在堆叠过程中，基础模型的预测结果会作为元模型（或元学习器）的输入。元模型的工作是利用基础模型的预测结果做出最终预测：

    ![Stacking](pic/img_64bd95775fcd9.svg)

    堆叠步骤显示了在训练数据上训练的三个不同模型的预测结果汇总。

    基础模型和元模型不一定是同一类型。例如，我们可以将决策树与支持向量机（[SVM](https://www.baeldung.com/cs/one-class-svm#support-vector-machines)）配对使用。

    具体步骤如下

    - 在训练数据的不同部分构建基础模型
    - 根据基础模型的预测结果训练元模型

    1. 堆叠的利与弊

        我们可以将堆叠总结如下：

        | 优点          | 缺点                   |
        |-------------|----------------------|
        | 集不同型号的优点于一身 | 可能需要较长的时间来训练和汇总      |
        | 提高整体精确度     | 不同类型模型的预测结果          |
        |             | 训练多个基本模型和一个元模型会增加复杂性 |

5. 套袋法、提升法和堆叠法的区别

    bagging、boosting 和 stacking 的主要区别在于方法、基础模型、子集选择、目标和模型组合：

    | 标准   | 装袋       | 提升        | 堆叠                 |
    |------|----------|-----------|--------------------|
    | 方法   | 并行训练弱模型  | 弱模型的序列训练  | 将多个模型的预测结果汇总为一个元模型 |
    | 基础模型 | 同质       | 同质        | 可以是异质的             |
    | 子集选择 | 带替换的随机抽样 | 无子集       | 无子集                |
    | 目标   | 减少方差     | 减少偏差      | 减少差异和偏差            |
    | 模型组合 | 多数表决或平均  | 加权多数表决或平均 | 使用 ML 模型           |

    选择使用哪种技术取决于总体目标和当前任务。当我们的目标是减少方差时，最好使用 Bagging，而减少偏差时则选择 boosting。如果目标是减少方差和偏差并提高整体性能，我们就应该使用堆叠技术。

6. 结论

    在本文中，我们概述了袋化、提升和堆叠。Bagging 可以并行地训练多个弱模型。提升法依次训练多个同质的弱模型，每个后继模型都会改善前继模型的误差。堆叠训练多个模型（可以是异构的），以获得一个元模型。

    选择哪种集合技术取决于目标和任务，因为所有这三种技术都旨在通过结合多个模型的力量来提高整体性能。
