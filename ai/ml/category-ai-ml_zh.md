# 机器学习

机器学习是一种让机器从数据中学习的技术。了解从数据集训练模型的各种技术。

- [机器学习的基本概念](./ml-fundamentals_zh.md)
- [ML：训练、验证和测试](./ml-train-validate-test_zh.md)

- [特征和标签的区别](./feature-vs-label_zh.md)
- [机器学习中的特征、参数和类](./features-parameters-classes-ml_zh.md)
- [什么是机器学习中的重要特征](./ml-feature-importance-zh.md)

- [直觉贝叶斯分类法的简单解释](../../core-concepts/math-logic/naive-bayes-classification_zh.md)

- [监督学习、半监督学习、无监督学习和强化学习简介](./machine-learning-intro-zh.md)

- [支持向量机 (SVM)](ml-support-vector-machines-zh.md)
- 卷积神经网络简介
- [热身学习率意味着什么？](ml-lr-warm-up-zh.md)
- [什么是检索增强生成（RAG）](retrieval-augmented-generation-zh.md)
- [如何进行线性判别分析？](lda-zh.md)
- [如何为SVM选择核类型？](svm-choose-kernel-zh.md)
- [Focal Loss简介](focal-loss-zh.md)
- [什么是强化学习中的贝尔曼算子？](bellman-operator-reinforcement-learning-zh.md)
- [径向基函数](rbf-neural-networks-zh.md)
- [GELU解释](./deep-learning/gelu-activation-function-zh.md)
- [什么是噪声对比估计损失？](./deep-learning/noise-contrastive-estimation-loss-zh.md)
- [如何计算分类器的VC维度？](vapnik-chervonenkis-dimension-computation-zh.md)
- [如何处理逻辑回归中的缺失数据？](logistic-regression-missing-data-zh.md)
- [如何用 SMOTE 处理不平衡数据？](synthetic-minority-oversampling-technique-zh.md)
- [如何绘制 Logistic 回归的决策边界？](draw-logistic-regression-decision-boundary-zh.md)
- [交叉熵和 KL Divergence 有什么区别？](cross-entropy-vs-kullback-leibler-divergence-zh.md)
- [饱和非线性](./deep-learning/saturating-non-linearities-zh.md)
- [什么是 Hopfield 网络？](hopfield-network-zh.md)
- [神经网络：阶梯卷积](neural-nets-strided-convolutions-zh.md)
- [与标准 PCA 相比，核 PCA 有哪些优势？](kernel-principal-component-analysis-zh.md)
- [人工智能图像生成器如何工作？](./deep-learning/ai-image-generation-gans-dalle-zh.md)
- [什么是TinyML？](tinyml-zh.md)
- [用于聚类的GMM](gaussian-mixture-models-clustering-zh.md)

- [数据质量解释](../data-science/data-quality-zh.md)

- [统计在机器学习中的重要性](ml-statistics-significance-zh.md)

- [如何提前停止？](early-stopping-regularization-zh.md)
- [高斯混合模型](gaussian-mixture-models-zh.md)

- [什么是奇异值分解？](svd-matrices-zh.md)

- [集合学习](ensemble-learning_zh.md)
- [机器学习中的装袋、提升和堆叠](bagging-boosting-stacking-ml-ensemble-models-zh.md)

- [尖峰神经网络简介](spiking-neural-networks-zh.md)

- [机器学习的Python](python-ml-programming-zh.md)

>> K-均值算法的缺点

- [从 RNN 到变换器](../rnns-transformers-nlp_zh.md)
- [机器学习：主动学习](ml-active-learning_zh.md)

>> 支持向量机中的 C 参数
>> 什么是温度，为什么要在 Softmax 中使用温度？
>> 自组织图如何工作？
>> 时间序列中在线离群点检测算法
>> 如何分析损失图与历时图？
>> 神经网络中的密集与稀疏概念
>> 作为矩阵-矩阵乘法的二维卷积
>> 懒惰学习与急切学习
>> DBSCAN 聚类： 它是如何工作的？
>> Q-Learning vs. Deep Q-Learning vs. Deep Q-Network
>> 什么是下游任务？
>> 什么是神经风格转移？
>> 自动化机器学习详解
>> 什么是联合学习？
>> 学习率预热是什么意思？
>> 三重丢失介绍
>> 神经网络： 汇集层
>> 词嵌入的维度
>> 什么是学分分配问题？
>> ADAM 优化器
>> PAC 学习理论的真正含义是什么？
>> 共现矩阵及其在 NLP 中的应用
>> 灵敏度和特异性
>> 硬投票与软投票分类器
>> 强化学习与最优控制的区别
>> 什么是独立成分分析（ICA）？
>> 参数模型与非参数模型的区别
>> 最大似然估计
>> 图形注意力网络
>> 稀疏编码神经网络
>> 卢昂注意与巴赫达瑙注意的区别
>> 铰链损失和逻辑损失的区别
>> 机器学习：如何为训练格式化图像
>> 机器学习：灵活和不灵活的模型
>> 什么是受限玻尔兹曼机？
>> 什么是数据湖？
>> 吉布斯采样简介
>> 一热编码解释
>> 机器学习中的特征选择
>> 迁移学习与元学习的区别
>> 为什么要使用替代损失

- [参数与超参数](./parameters-vs-hyperparameters_zh.md)

>> 什么是免费午餐定理？
>> 神经网络中的主干是什么意思？
>> 什么是基于内容的图像检索？
>> 偏差和误差的区别
>> 在线学习与离线学习
>> 什么是一类 SVM 及其工作原理？
>> 数据扩充

- [什么是神经网络中的微调？](./fine-tuning-nn-zh.md)

>> 随机森林与极随机化树
>> 如何使用 Gabor 滤波器生成机器学习特征
>> 随机样本共识解释
>> 预训练神经网络意味着什么？
>> 神经网络： 什么是权重衰减损失？
>> 利用威胁空间搜索赢得五子棋
>> 神经网络： Conv 层和 FC 层的区别

- [神经网络：二进制输入与离散输入与连续输入](./binary-vs-discrete-vs-continuous-inputs_zh.md)

>> 梯度、随机和迷你批量梯度下降之间的区别
>> 规模不变特征变换
>> 什么是回归器？
>> 多层感知器与深度神经网络
>> 机器学习：什么是消融研究？
>> 剪影图
>> 决策树中的节点杂质
>> 0-1 损失函数解释
>> 缺失数据和稀疏数据的区别
>> 反向传播网络和前馈网络的区别

- 深度学习：交叉验证：K-Fold vs. 一票否决

>> 非政策强化学习与政策强化学习
>> 比较 Naïve Bayes 和 SVM 在文本分类中的应用
>> 自然语言处理中的递归神经网络与递归神经网络
>> 什么是神经网络中的 "瓶颈"？
>> 神经网络中的隐藏层
>> 监督学习和非监督学习的实际例子
>> 遗传算法在现实世界中的应用
>> 决策树与随机森林
>> 什么是机器学习中的归纳偏差？
>> 人工智能、机器学习、统计学和数据挖掘之间有什么区别？
>> 自我监督学习简介
>> 深度学习中的潜空间
>> 自编码器详解
>> 成本、损失和目标函数的区别
>> 激活函数： Sigmoid 与 Tanh
>> 对比学习简介
>> 梯度提升树 vs. 随机森林
>> 机器学习核背后的直觉
>> 图像比较算法
>> 图像处理： 遮挡
>> 机器学习中的信息增益
>> 如何在神经网络中使用 K 折交叉验证？
>> 生成式对抗网络简介
>> K-Means 分类法
>> Q-Learning vs. SARSA
>> SGD 与反向传播的区别
>> 神经网络中的线性可分离数据
>> 随机森林中树的深度和数量的影响

- [双向和单向LSTM的区别](./bidirectional-vs-unidirectional-lstm_zh.md)

>> 学习率与批量大小的关系
>> Word2vec 单词嵌入操作： 添加、串联还是平均单词向量？
>> 机器学习中的漂移、异常和新颖性
>> 马尔可夫决策过程： 值迭代是如何工作的？
>> 决策树 vs. Naive Bayes 分类器
>> 机器学习中的准确率 vs AUC
>> 贝叶斯网络
>> 机器学习中的偏差
>> 主题建模中的一致性得分是好是坏？
>> 马尔科夫链聊天机器人如何工作？
>> 机器学习中的分层抽样
>> 异常值检测与处理
>> 选择学习率
>> 机器学习中的欠拟合和过拟合
>> "20个问题"人工智能算法如何工作？
>> 如何计算线性回归中的正则化参数
>> 如何从标记词的 word2vec 中获取句子向量
>> Q-Learning vs. 动态编程
>> 文本分类的特征选择和缩减
>> SVM 和感知器的区别
>> 为什么迷你批量比包含所有训练数据的单一 "批量"更好？
>> 期望最大化（EM）技术的直观解释
>> 时间序列中的模式识别
>> 如何创建智能聊天机器人？

- [随机森林的袋外误差](./random-forests-out-of-bag-error_zh.md)

>> 如何计算 CNN 的感受野大小

- k近邻和高维数据

>> 在 SVM 中使用硬边际与软边际
>> 强化学习中的值迭代与策略迭代
>> 如何将文本序列转换为向量
>> 实例规范化与批量规范化
>> SVM 中精度与支持向量数量之间的权衡

- [开源人工智能引擎](./open-source-ai-engines_zh.md)
- 开放源码神经网络库

- [变换器文本嵌入](transformer-text-embeddings_zh.md)

>> 生成算法与判别算法
>> SVM 中为什么要进行特征缩放？
>> 线性回归中的归一化与标准化
>> 词嵌入： CBOW 与 Skip-Gram
>> 字符串相似度量： 基于序列
>> 如何提高 Naive Bayes 分类性能？
>> 丑小鸭定理
>> 使用 Word2Vec 进行主题建模
>> 表格特征标准化
>> 字符串相似度量： 标记方法
>> 逻辑回归中的梯度下降方程

- [相关特征和分类精度](./correlation-classification-algorithms_zh.md)

>> 弱监督学习
>> 解释机器学习模型的损失和准确性
>> 将数据集拆分为训练集和测试集
>> 解决 K 臂匪徒问题
>> 神经网络的时代
>> Epsilon-Greedy Q-learning
>> 神经网络中权重的随机初始化

- [使用支持向量机进行多分类](./svm-multiclass-classification_zh.md)

>> 利用神经网络进行强化学习

- [什么是交叉熵？](./cross-entropy_zh.md)

>> 神经网络与 SVM 相比的优缺点
>> Top-N 精度指标
>> 神经网络架构： 选择隐藏层数量和大小的标准
>> 情感分析的训练数据

- [分类和聚类的区别](./ml-classification-vs-clustering_zh.md)

>> 多类分类的 F-1 分数
>> 什么是强化学习中的策略？
>> SVM 与神经网络
>> 人工神经网络的输入规范化
>> 什么是机器学习中的学习曲线？
>> 分类模型评估简介
>> 为什么逻辑回归的代价函数是对数表达式？
>> 线性回归与逻辑回归
>> 神经网络中的偏差
>> 如何计算两篇文本文档的相似度？
>> 分割数据集之前还是之后的数据归一化？

- [特征缩放](./feature-scaling_zh.md)

## [Machine Learning](https://www.baeldung.com/cs/category/ai/ml)

- Cross-Validation: K-Fold vs. Leave-One-Out
- [Support Vector Machines (SVM)](https://www.baeldung.com/cs/ml-support-vector-machines)
- Introduction to Convolutional Neural Networks
- [What Does Learning Rate Warm-up Mean?](https://www.baeldung.com/cs/ml-lr-warm-up)
- [What Is Retrieval-Augmented Generation (RAG)?](https://www.baeldung.com/cs/retrieval-augmented-generation)
- [How to Perform Linear Discriminant Analysis?](https://www.baeldung.com/cs/lda)
- [How to Select the Type of Kernel for a SVM?](https://www.baeldung.com/cs/svm-choose-kernel)
- [An Introduction to Focal Loss](https://www.baeldung.com/cs/focal-loss)
- [What is the Bellman Operator in Reinforcement Learning?](https://www.baeldung.com/cs/bellman-operator-reinforcement-learning)
- [Radial Basis Function](https://www.baeldung.com/cs/rbf-neural-networks)
- GELU Explained
- What Is Noise Contrastive Estimation Loss?
- ReLU vs. LeakyReLU vs. PReLU
- [How to Calculate the VC-Dimension of a Classifier?](https://www.baeldung.com/cs/vapnik-chervonenkis-dimension-computation)
- [How to Handle Missing Data in Logistic Regression?](https://www.baeldung.com/cs/logistic-regression-missing-data)
- [How to Handle Unbalanced Data With SMOTE?](https://www.baeldung.com/cs/synthetic-minority-oversampling-technique)
- [How to Plot Logistic Regression’s Decision Boundary?](https://www.baeldung.com/cs/draw-logistic-regression-decision-boundary)
- [What’s the Difference Between Cross-Entropy and KL Divergence?](https://www.baeldung.com/cs/cross-entropy-vs-kullback-leibler-divergence)
- Saturating Non-Linearities
- [What Is A Hopfield Network?](https://www.baeldung.com/cs/hopfield-network)
- [Neural Networks: Strided Convolutions](https://www.baeldung.com/cs/neural-nets-strided-convolutions)
- [What Are the Advantages of Kernel PCA Over Standard PCA?](https://www.baeldung.com/cs/kernel-principal-component-analysis)
- How Do AI Image Generators Work?
- [What Is TinyML?]
- [GMMs for Clustering](https://www.baeldung.com/cs/gaussian-mixture-models-clustering)
- Data Quality Explained
- [Importance of Statistics in Machine Learning](https://www.baeldung.com/cs/ml-statistics-significance)
- [How to Do Early Stopping?](https://www.baeldung.com/cs/early-stopping-regularization)
- [Gaussian Mixture Models](https://www.baeldung.com/cs/gaussian-mixture-models)
- [Python for Machine Learning](https://www.baeldung.com/cs/python-ml-programming)
- [What Is Singular Value Decomposition?](https://www.baeldung.com/cs/svd-matrices)
- [Bagging, Boosting, and Stacking in Machine Learning](https://www.baeldung.com/cs/bagging-boosting-stacking-ml-ensemble-models)
- [Introduction to Spiking Neural Networks](https://www.baeldung.com/cs/spiking-neural-networks)

>> The Drawbacks of K-Means Algorithm

- From RNNs to Transformers

>> Machine Learning: Analytical Learning
>> The C Parameter in Support Vector Machines

- What Is and Why Use Temperature in Softmax?

>> How Do Self-Organizing Maps Work?
>> Algorithm for Online Outlier Detection in Time Series
>> How to Analyze Loss vs. Epoch Graphs?
>> The Concepts of Dense and Sparse in the Context of Neural Networks
>> 2D Convolution as a Matrix-Matrix Multiplication
>> Lazy vs. Eager Learning
>> DBSCAN Clustering: How Does It Work?
>> Q-Learning vs. Deep Q-Learning vs. Deep Q-Network
>> What Are Downstream Tasks?
>> What Is Neural Style Transfer?
>> Automated Machine Learning Explained
>> What Is Federated Learning?
>> What Does Learning Rate Warm-up Mean?
>> Introduction to Triplet Loss
>> Neural Networks: Pooling Layers
>> Dimensionality of Word Embeddings
>> What Is the Credit Assignment Problem?
>> ADAM Optimizer
>> What Does PAC Learning Theory Really Mean?
>> Co-occurrence Matrices and Their Uses in NLP
>> Sensitivity and Specificity
>> Hard vs. Soft Voting Classifiers
>> Difference Between Reinforcement Learning and Optimal Control
>> What Is Independent Component Analysis (ICA)?
>> Differences Between a Parametric and Non-parametric Model
>> Maximum Likelihood Estimation
>> Graph Attention Networks
>> Sparse Coding Neural Networks
>> Differences Between Luong Attention and Bahdanau Attention
>> Differences Between Hinge Loss and Logistic Loss
>> Machine Learning: How to Format Images for Training
>> Machine Learning: Flexible and Inflexible Models
>> What Are Restricted Boltzmann Machines?
>> What Is a Data Lake?
>> Introduction to Gibbs Sampling
>> One-Hot Encoding Explained
>> Feature Selection in Machine Learning
>> Differences Between Transfer Learning and Meta-Learning
>> Why Use a Surrogate Loss

- Parameters vs. Hyperparameters

>> What Is the No Free Lunch Theorem?
>> What Does Backbone Mean in Neural Networks?
>> What Is Content-Based Image Retrieval?
>> Differences Between Bias and Error
>> Online Learning vs. Offline Learning
>> What Is One Class SVM and How Does It Work?

- What Is Feature Importance in Machine Learning?
- Data Augmentation
- What Is Fine-Tuning in Neural Networks?

>> Random Forest vs. Extremely Randomized Trees
>> How to Use Gabor Filters to Generate Features for Machine Learning
>> Random Sample Consensus Explained
>> What Does Pre-training a Neural Network Mean?
>> Neural Networks: What Is Weight Decay Loss?
>> Win Gomoku with Threat Space Search
>> Neural Networks: Difference Between Conv and FC Layers

- Neural Networks: Binary vs. Discrete vs. Continuous Inputs

>> Differences Between Gradient, Stochastic and Mini Batch Gradient Descent
>> Scale-Invariant Feature Transform
>> What Is a Regressor?
>> Multi-Layer Perceptron vs. Deep Neural Network
>> Machine Learning: What Is Ablation Study?
>> Silhouette Plots
>> Node Impurity in Decision Trees
>> 0-1 Loss Function Explained
>> Differences Between Missing Data and Sparse Data
>> Differences Between Backpropagation and Feedforward Networks
>> Off-policy vs. On-policy Reinforcement Learning
>> Comparing Naïve Bayes and SVM for Text Classification
>> Recurrent vs. Recursive Neural Networks in Natural Language Processing
>> What Are “Bottlenecks” in Neural Networks?
>> Hidden Layers in a Neural Network
>> Real-Life Examples of Supervised Learning and Unsupervised Learning
>> Real-World Uses for Genetic Algorithms
>> Decision Trees vs. Random Forests
>> What Is Inductive Bias in Machine Learning?
>> What Is the Difference Between Artificial Intelligence, Machine Learning, Statistics, and Data Mining?
>> An Introduction to Self-Supervised Learning
>> Latent Space in Deep Learning
>> Autoencoders Explained
>> Difference Between the Cost, Loss, and the Objective Function
>> Activation Functions: Sigmoid vs Tanh
>> An Introduction to Contrastive Learning
>> Gradient Boosting Trees vs. Random Forests

- Basic Concepts of Machine Learning

>> Intuition Behind Kernels in Machine Learning
>> Algorithms for Image Comparison
>> Image Processing: Occlusions
>> Information Gain in Machine Learning
>> How to Use K-Fold Cross-Validation in a Neural Network?
>> An Introduction to Generative Adversarial Networks
>> K-Means for Classification

- ML: Train, Validate, and Test

>> Q-Learning vs. SARSA
>> Differences Between SGD and Backpropagation
>> Linearly Separable Data in Neural Networks
>> The Effects of the Depth and Number of Trees in a Random Forest

- Differences Between Bidirectional and Unidirectional LSTM

- Features, Parameters and Classes in Machine Learning

>> Relation Between Learning Rate and Batch Size
>> Word2vec Word Embedding Operations: Add, Concatenate or Average Word Vectors?
>> Drift, Anomaly, and Novelty in Machine Learning
>> Markov Decision Process: How Does Value Iteration Work?

- Ensemble Learning

>> Decision Tree vs. Naive Bayes Classifier
>> Accuracy vs AUC in Machine Learning
>> Bayesian Networks
>> Biases in Machine Learning
>> When Coherence Score Is Good or Bad in Topic Modeling?
>> How Do Markov Chain Chatbots Work?
>> Stratified Sampling in Machine Learning

- Outlier Detection and Handling

>> Choosing a Learning Rate
>> Underfitting and Overfitting in Machine Learning
>> How Do “20 Questions” AI Algorithms Work?
>> How to Calculate the Regularization Parameter in Linear Regression
>> How to Get Vector for A Sentence From Word2vec of Tokens
>> Q-Learning vs. Dynamic Programming
>> Feature Selection and Reduction for Text Classification
>> Difference Between a SVM and a Perceptron
>> Why Mini-Batch Size Is Better Than One Single “Batch” With All Training Data
>> Intuitive Explanation of the Expectation-Maximization (EM) Technique
>> Pattern Recognition in Time Series
>> How to Create a Smart Chatbot?

- Open-Source AI Engines

- Out-of-bag Error in Random Forests

>> How to Calculate Receptive Field Size in CNN

- k-Nearest Neighbors and High Dimensional Data

>> Using a Hard Margin vs. Soft Margin in SVM
>> Value Iteration vs. Policy Iteration in Reinforcement Learning
>> How To Convert a Text Sequence to a Vector

- Instance vs Batch Normalization

>> Trade-offs Between Accuracy and the Number of Support Vectors in SVMs

- Open Source Neural Network Libraries
- Transformer Text Embeddings

>> Generative vs. Discriminative Algorithms
>> Why Feature Scaling in SVM?
>> Normalization vs Standardization in Linear Regression
>> Word Embeddings: CBOW vs Skip-Gram
>> String Similarity Metrics: Sequence Based
>> How to Improve Naive Bayes Classification Performance?
>> Ugly Duckling Theorem
>> Topic Modeling with Word2Vec
>> Normalize Features of a Table
>> String Similarity Metrics: Token Methods
>> Gradient Descent Equation in Logistic Regression

- Correlated Features and Classification Accuracy

>> Weakly Supervised Learning
>> Interpretation of Loss and Accuracy for a Machine Learning Model
>> Splitting a Dataset into Train and Test Sets
>> Solving the K-Armed Bandit Problem
>> Epoch in Neural Networks
>> Epsilon-Greedy Q-learning
>> Random Initialization of Weights in a Neural Network

- Multiclass Classification Using Support Vector Machines

>> Reinforcement Learning with Neural Network

- What Is Cross-Entropy?

>> Advantages and Disadvantages of Neural Networks Against SVMs
>> Top-N Accuracy Metrics

- Neural Network Architecture: Criteria for Choosing the Number and Size of Hidden Layers

>> Training Data for Sentiment Analysis
>> Differences Between Classification and Clustering
>> F-1 Score for Multi-Class Classification
>> What Is a Policy in Reinforcement Learning?
>> SVM Vs Neural Network

- Normalizing Inputs for an Artificial Neural Network

>> What Is a Learning Curve in Machine Learning?
>> Introduction to the Classification Model Evaluation
>> Why Does the Cost Function of Logistic Regression Have a Logarithmic Expression?
>> Linear Regression vs. Logistic Regression
>> Bias in Neural Networks
>> How to Compute the Similarity Between Two Text Documents?

- Difference Between a Feature and a Label

>> Data Normalization Before or After Splitting a Data Set?

- A Simple Explanation of Naive Bayes Classification
- Feature Scaling
- Introduction to Supervised, Semi-supervised, Unsupervised and Reinforcement Learning
