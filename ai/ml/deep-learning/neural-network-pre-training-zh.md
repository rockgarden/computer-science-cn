# 预训练神经网络意味着什么？

[深度学习](https://www.baeldung.com/cs/category/ai/deep-learning) [机器学习](https://www.baeldung.com/cs/category/ai/ml)

[神经网络](https://www.baeldung.com/cs/tag/neural-networks) [训练](https://www.baeldung.com/cs/tag/training)

1. 简介

    在本教程中，我们将回顾[神经网络](https://www.baeldung.com/cs/binary-vs-discrete-vs-continuous-inputs#definitions)中的预训练：它是什么、如何完成以及在哪些方面使用。最后，我们将探讨预训练神经网络的优点和缺点。

2. 预训练

    简单来说，[预训练](https://medium.com/ai³-theory-practice-business/what-is-pre-training-in-nlp-introducing-5-key-technologies-455c54933054)神经网络是指首先在一个任务或数据集上训练一个模型。然后利用训练中获得的参数或模型，在不同的任务或数据集上训练另一个模型。这就为模型提供了先机，而不是从零开始。

    假设我们想对猫和狗的数据集进行分类。我们为这个分类任务建立了一个机器学习模型 ml。一旦 ml 完成训练，我们就将其连同所有参数一起保存起来。现在，假设我们要完成另一项任务：物体检测。我们不需要从头开始训练一个新模型，而是在物体检测数据集上使用 ml。我们将此称为预训练。

3. 如何进行预训练？

    预训练神经网络最关键的一点是手头的任务。具体来说，模型最初学习的任务必须与模型将来要执行的任务相似。我们不能先训练一个天气预报模型，然后再将其用于物体检测。

    现在，预训练神经网络需要[四个基本步骤](https://martin-thoma.com/ml-glossary/)：

    - 我们有一个机器学习模型 mm 以及数据集 A 和 B
    - 用数据集 A 训练毫米
    - 在数据集 B 上训练模型之前，用在 A 上训练的模型初始化 mm 的部分参数
    - 在 B 数据集上训练毫米

4. 预训练的应用

    预训练的应用可分为三类： [迁移学习](https://www.baeldung.com/cs/transfer-learning-vs-domain-adoption#introduction)、分类和[特征提取](https://www.mathworks.com/help/deeplearning/ug/pretrained-convolutional-neural-networks.html#bvnkti1)。

    1. 迁移学习

        迁移学习是指将从一个机器学习问题中获得的知识用于另一个问题。例如，利用从猫/狗检测中获得的知识来检测建筑物。迁移学习的主要内容是使用预先训练好的模型来收集一项任务中的知识，并将其应用到其他任务中。

        最重要的是，迁移学习被认为是人工智能开发人员的一大飞跃，因为它能让我们更快、更高效地开发应用程序。

    2. 分类

        另外，预训练模型也可应用于分类任务，例如图像分类。[图像分类](https://www.tensorflow.org/lite/examples/image_classification/overview)指的是识别给定图像代表什么的任务。目前，有许多预先训练好的模型专门用于图像分类。这些模型已在大型图像数据集上进行过训练，因此可用于任何图像分类任务。

    3. 特征提取

        相反，使用预训练模型进行特征提取需要使用预训练模型来提取有意义的特征。提取的特征随后可用作另一个模型的输入。

5. 预训练的优点和缺点

    预训练神经网络模型可实现高效的模型开发。虽然使用预训练模型大多是有益的，但也存在一些缺点。让我们在接下来的章节中回顾一下这些优缺点：

    1. 优点

        如果我们首先考虑优点，那么预训练最显著的优点就是易于使用。假设我们有一个机器学习任务要做。我们需要做的就是找到一个在类似任务中训练过的预训练模型，并将其应用到我们正在处理的任务中。没有必要从头开始构建模型。

        预训练可以快速优化模型。这意味着，如果使用预先训练好的模型，模型可以更快地达到最佳性能。与从头开始相比，如果一个模型先知道哪些参数可能取得好的结果，就能更快地进行优化。

        此外，预训练模型还有一个好处，就是不需要像从头开始建立模型那样多的数据。这是因为迄今为止互联网上的大多数预训练模型都是在极其庞大的数据集上训练出来的。因此，在不同的任务中使用这样的模型需要更少的数据来收敛。

    2. 缺点

        预训练虽然有益，但在应用时必须谨慎。首先，我们并不总能获得预训练模型所取得的良好结果。有几个因素可能会导致这种情况。例如，使用完全不同领域的数据集可能无法获得相同的结果。此外，网络参数、[训练-测试分割比率](https://www.baeldung.com/cs/ml-train-validate-test)和用于训练的硬件也是决定性因素。

        此外，微调预训练模型也是一项艰巨的任务。它们需要时间和 CPU 资源才能有效地进行微调。

6. 预训练模型实例

    迄今为止，工业界和学术界已经使用了多种预训练模型。每种模型都能达到不同的性能水平，并用于不同的任务。一些著名的[计算机视觉](https://www.baeldung.com/cs/deep-cnn-design#introduction)模型有：

    - VGG-16
    - ResNet50
    - Inceptionv3
    - EfficientNet

    一些用于自然语言处理 (NLP) 任务的流行预训练模型：

    - GPT-3
    - BERT
    - ELMo
    - XLNet
    - ALBERT

    值得注意的是，这些预训练模型大多可以在 TensorFlow、Keras 和 PyTorch 等流行的机器学习库中找到。

7. 结论

    在本教程中，我们回顾了神经网络的预训练。预训练神经网络模型只是在一个任务中训练好的模型，然后用于另一个任务。要对神经网络进行预训练，我们必须有一个初始模型和一个要训练的数据集。预训练模型的三个主要应用领域是迁移学习、特征提取和分类。

    总之，预训练模型是构建人工智能应用的一种快速、高效的方法，但并不总能保证在不同任务中具有相同的性能。
