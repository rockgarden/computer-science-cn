# 深度学习

深度学习是一种利用神经网络进行机器学习的技术。了解训练、优化和使用多层神经网络的各种技术。

- [卷积神经网络简介](/ai/ai-convolutional-neural-networks-zh.md)
- [ReLU vs. LeakyReLU vs. PReLU](relu-vs-leakyrelu-vs-prelu-zh.md)
- [饱和非线性](saturating-non-linearities-zh.md)
- [人工智能图像生成器如何工作？](ai-image-generation-gans-dalle-zh.md)
- [利用LSTM防止梯度消失问题](lstm-vanishing-gradient-prevention-zh.md)
- [机器学习与深度学习](machine-learning-vs-deep-learning-zh.md)
- [图神经网络简介](ml-gnn-zh.md)
- [深度伪造简介](deepfakes-ai-zh.md)
- [强化学习中的确定性策略与随机策略](rl-deterministic-vs-stochastic-policies-zh.md)
- [大型语言模型简介](large-language-models-zh.md)
- [Softmax中的温度是什么，为什么要使用温度？](softmax-temperature-zh.md)
- [什么是不可训练参数？](non-trainable-parameter-zh.md)
- [Epoch or Episode：理解深度强化学习中的术语](epoch-vs-episode-reinforcement-learning-zh.md)
- [Q-Learning vs. Deep Q-Learning vs. Deep Q-Network](../../ml/q-learning-vs-deep-q-learning-vs-deep-q-network-zh.md)
- [什么是端到端深度学习？](end-to-end-deep-learning-zh.md)
- [连体网络如何在图像识别中发挥作用？](siamese-networks-zh.md)
- [深度神经网络：填充](deep-neural-networks-padding-zh.md)
- [单次检测器（SSD）](../../computer-vision/ssd-zh.md)
- [什么是神经网络中的Maxout？](maxout-neural-networks-zh.md)
- [循环神经网络](recurrent-neural-networks-zh.md)
- [神经网络中的嵌入层是什么？](neural-nets-embedding-layers-zh.md)
- [为什么使用替代损失？](surrogate-loss-zh.md)
- [生成对抗网络：判别器损失和生成器损失](gan-loss-zh.md)
- [神经网络中的主干是什么意思？](neural-network-backbone-zh.md)
- [卷积网络中的通道是什么？](cnn-channels-zh.md)
- [偏差与误差的区别](bias-vs-error-zh.md)
- [实例分割与语义分割](../../computer-vision/instance-semantic-segmentation-cnn-zh.md)
- [在线学习与离线学习](online-vs-offline-learning-zh.md)
- [数据扩充](ml-data-augmentation-zh.md)
- [随机样本共识解释](ransac-zh.md)
- [预训练神经网络意味着什么？](neural-network-pre-training-zh.md)
- [神经网络：什么是权重衰减损失？](neural-networks-weight-decay-loss-zh.md)
- [神经网络：Conv层和FC层的区别](neural-networks-conv-fc-layers-zh.md)
- [究竟什么是N-Gram？](n-gram-zh.md)
- [多层感知器vs.深度神经网络](mlp-vs-dnn-zh.md)
- [无模型强化学习与基于模型的强化学习](ai-model-free-vs-model-based-zh.md)
- [0-1损失函数解释](ai-0-1-loss-function-zh.md)
- [反向传播与前馈网络的区别](neural-networks-backprop-vs-feedforward-zh.md)
- [交叉验证：K-折叠vs.一票否决](cross-validation-k-fold-loo-zh.md)
- [非策略与策略强化学习](off-policy-vs-on-policy-zh.md)
- [神经网络反向传播中的偏差更新](deep-learning-bias-backpropagation-zh.md)
- [自然语言处理中的递归与递归神经网络](networks-in-nlp-zh.md)
- [什么是神经网络中的"瓶颈"？](neural-network-bottleneck-zh.md)
- [卷积神经网络与普通神经网络](convolutional-vs-regular-nn-zh.md)
- [物体检测中的平均精度](../../computer-vision/ml-map-object-detection-zh.md)
- [神经网络中的隐藏层](hidden-layers-neural-network-zh.md)
- [监督学习和非监督学习的真实案例](examples-supervised-unsupervised-learning-zh.md)
- [遗传算法在现实世界中的应用](genetic-algorithms-applications-zh.md)
- [变异自动编码器中的重参数化技巧](../../computer-vision/vae-reparameterization-zh.md)
- [什么是机器学习中的归纳偏差？](ml-inductive-bias-zh.md)
- [深度学习中的潜空间](dl-latent-space-zh.md)
- [自编码器详解](autoencoders-explained-zh.md)
- [激活函数：Sigmoid vs Tanh](sigmoid-vs-tanh-functions-zh.md)
- [对比学习简介](contrastive-learning-zh.md)
>> 机器学习中内核背后的直觉
>> 图像比较算法
>> 图像处理：遮挡
>> 生成模型的应用
>> 计算卷积层的输出大小
- [生成对抗网络简介](generative-adversarial-networks-zh.md)
>> 神经网络中的线性可分离数据
>> 使用 GAN 进行数据扩充
>> 学习率与批量大小的关系
- [Word2vec单词嵌入操作：添加、串联还是平均单词向量？](word2vec-word-embeddings-zh.md)
>> 异常值检测和处理
>> 文本分类的特征选择和缩减
>> 为什么迷你批量比包含所有训练数据的单一"批量"更好？
- [如何创建智能聊天机器人？](smart-chatbots-zh.md)
>> 如何计算 CNN 的感知场大小？
>> k 近邻和高维数据
>> 在 SVM 中使用硬边际与软边际
>> 强化学习中的值迭代与策略迭代
>> 如何将文本序列转换为向量
>> 实例归一化 vs 批量归一化
>> SVM 中精度与支持向量数量之间的权衡
- [开源神经网络库](ml-open-source-libraries-zh.md)
>> 词嵌入： CBOW vs Skip-Gram
>> 自然语言处理的编码器-解码器模型
>> 神经网络中的纪元
>> 神经网络中权重的随机初始化
>> 卷积神经网络中的批量归一化
>> 神经网络与 SVM 相比的优缺点
>> 神经网络架构： 选择隐藏层的数量和大小的标准
>> 情感分析的训练数据
>> 多类分类的 F-1 分数
>> 什么是强化学习中的策略？
>> 梯度下降和梯度上升有什么区别？
>> 人工神经网络输入归一化
>> 卷积神经网络简介
>> 神经网络中的偏差
>> 理解CNN中的维度

## Deep Learning

- [Introduction to Convolutional Neural Networks](https://www.baeldung.com/cs/ai-convolutional-neural-networks)
- [ReLU vs. LeakyReLU vs. PReLU](https://www.baeldung.com/cs/relu-vs-leakyrelu-vs-prelu)
- [Saturating Non-Linearities](https://www.baeldung.com/cs/saturating-non-linearities)
- [How Do AI Image Generators Work?](https://www.baeldung.com/cs/ai-image-generation-gans-dalle)
- [Prevent the Vanishing Gradient Problem with LSTM](https://www.baeldung.com/cs/lstm-vanishing-gradient-prevention)
- [Machine Learning vs. Deep Learning](https://www.baeldung.com/cs/machine-learning-vs-deep-learning)
- [An Introduction to Graph Neural Networks](https://www.baeldung.com/cs/ml-gnn)
- [An Introduction to Deepfakes](https://www.baeldung.com/cs/deepfakes-ai)
- [Deterministic vs. Stochastic Policies in Reinforcement Learning](https://www.baeldung.com/cs/rl-deterministic-vs-stochastic-policies)
- [Introduction to Large Language Models](https://www.baeldung.com/cs/large-language-models)
- [What Is and Why Use Temperature in Softmax?](https://www.baeldung.com/cs/softmax-temperature)
- [What’s a Non-trainable Parameter?](https://www.baeldung.com/cs/non-trainable-parameter)
- [Epoch or Episode: Understanding Terms in Deep Reinforcement Learning](https://www.baeldung.com/cs/epoch-vs-episode-reinforcement-learning)
- [Q-Learning vs. Deep Q-Learning vs. Deep Q-Network](https://www.baeldung.com/cs/q-learning-vs-deep-q-learning-vs-deep-q-network)
- [What Is End-to-End Deep Learning?](https://www.baeldung.com/cs/end-to-end-deep-learning)
- [How Do Siamese Networks Work in Image Recognition?](https://www.baeldung.com/cs/siamese-networks)
- [Deep Neural Networks: Padding](https://www.baeldung.com/cs/deep-neural-networks-padding)
- [Single Shot Detectors (SSDs)](https://www.baeldung.com/cs/ssd)
- [What Is Maxout in a Neural Network?](https://www.baeldung.com/cs/maxout-neural-networks)
- [Recurrent Neural Networks](https://www.baeldung.com/cs/recurrent-neural-networks)
- [What Are Embedding Layers in Neural Networks?](https://www.baeldung.com/cs/neural-nets-embedding-layers)
- [Why Use a Surrogate Loss](https://www.baeldung.com/cs/surrogate-loss)
- [Generative Adversarial Networks: Discriminator’s Loss and Generator’s Loss](https://www.baeldung.com/cs/gan-loss)
- [What Does Backbone Mean in Neural Networks?](https://www.baeldung.com/cs/neural-network-backbone)
- [What Are Channels in Convolutional Networks?](https://www.baeldung.com/cs/cnn-channels)
- [Differences Between Bias and Error](https://www.baeldung.com/cs/bias-vs-error)
- Instance Segmentation vs. Semantic Segmentation
- [Online Learning vs. Offline Learning](https://www.baeldung.com/cs/online-vs-offline-learning)
- [Data Augmentation](https://www.baeldung.com/cs/ml-data-augmentation)
- [Random Sample Consensus Explained](https://www.baeldung.com/cs/ransac)
- [What Does Pre-training a Neural Network Mean?](https://www.baeldung.com/cs/neural-network-pre-training)
- [Neural Networks: What Is Weight Decay Loss?](https://www.baeldung.com/cs/neural-networks-weight-decay-loss)
- [Neural Networks: Difference Between Conv and FC Layers](https://www.baeldung.com/cs/neural-networks-conv-fc-layers)
- [What Exactly Is an N-Gram?](https://www.baeldung.com/cs/n-gram)
- [Multi-Layer Perceptron vs. Deep Neural Network](https://www.baeldung.com/cs/mlp-vs-dnn)
- [Model-free vs. Model-based Reinforcement Learning](https://www.baeldung.com/cs/ai-model-free-vs-model-based)
- [0-1 Loss Function Explained](https://www.baeldung.com/cs/ai-0-1-loss-function)
- [Differences Between Backpropagation and Feedforward Networks](https://www.baeldung.com/cs/neural-networks-backprop-vs-feedforward)
- [Cross-Validation: K-Fold vs. Leave-One-Out](https://www.baeldung.com/cs/cross-validation-k-fold-loo)
- [Off-policy vs. On-policy Reinforcement Learning](https://www.baeldung.com/cs/off-policy-vs-on-policy)
- [Bias Update in Neural Network Backpropagation](https://www.baeldung.com/cs/deep-learning-bias-backpropagation)
- [Recurrent vs. Recursive Neural Networks in Natural Language Processing](https://www.baeldung.com/cs/networks-in-nlp)
- [What Are “Bottlenecks” in Neural Networks?](https://www.baeldung.com/cs/neural-network-bottleneck)
- [Convolutional Neural Network vs. Regular Neural Network](https://www.baeldung.com/cs/convolutional-vs-regular-nn)
- [Mean Average Precision in Object Detection](../../computer-vision/README-zh.md)
- [Hidden Layers in a Neural Network](https://www.baeldung.com/cs/hidden-layers-neural-network)
- [Real-Life Examples of Supervised Learning and Unsupervised Learning](https://www.baeldung.com/cs/examples-supervised-unsupervised-learning)
- [Real-World Uses for Genetic Algorithms](https://www.baeldung.com/cs/genetic-algorithms-applications)
- [The Reparameterization Trick in Variational Autoencoders](../../computer-vision/README-zh.md)
- [What Is Inductive Bias in Machine Learning?](https://www.baeldung.com/cs/ml-inductive-bias)
- [Latent Space in Deep Learning](https://www.baeldung.com/cs/dl-latent-space)
- [Autoencoders Explained](https://www.baeldung.com/cs/autoencoders-explained)
- [Activation Functions: Sigmoid vs Tanh](https://www.baeldung.com/cs/sigmoid-vs-tanh-functions)
- [An Introduction to Contrastive Learning](https://www.baeldung.com/cs/contrastive-learning)
>> Intuition Behind Kernels in Machine Learning
>> Algorithms for Image Comparison
>> Image Processing: Occlusions
>> Applications of Generative Models
>> Calculate the Output Size of a Convolutional Layer
- [An Introduction to Generative Adversarial Networks](https://www.baeldung.com/cs/generative-adversarial-networks)
>> Linearly Separable Data in Neural Networks
>> Using GANs for Data Augmentation
>> Relation Between Learning Rate and Batch Size
- [Word2vec Word Embedding Operations: Add, Concatenate or Average Word Vectors?](https://www.baeldung.com/cs/word2vec-word-embeddings)
>> Outlier Detection and Handling
Probability and Statistics
>> Feature Selection and Reduction for Text Classification
>> Why Mini-Batch Size Is Better Than One Single “Batch” With All Training Data
- [How to Create a Smart Chatbot?](https://www.baeldung.com/cs/smart-chatbots)
>> How to Calculate Receptive Field Size in CNN
>> k-Nearest Neighbors and High Dimensional Data
>> Using a Hard Margin vs. Soft Margin in SVM
>> Value Iteration vs. Policy Iteration in Reinforcement Learning
>> How To Convert a Text Sequence to a Vector
>> Instance vs Batch Normalization
>> Trade-offs Between Accuracy and the Number of Support Vectors in SVMs
- [Open Source Neural Network Libraries](https://www.baeldung.com/cs/ml-open-source-libraries)
>> Word Embeddings: CBOW vs Skip-Gram
>> Encoder-Decoder Models for Natural Language Processing
>> Epoch in Neural Networks
>> Random Initialization of Weights in a Neural Network
>> Batch Normalization in Convolutional Neural Networks
>> Advantages and Disadvantages of Neural Networks Against SVMs
>> Neural Network Architecture: Criteria for Choosing the Number and Size of Hidden Layers
>> Training Data for Sentiment Analysis
>> F-1 Score for Multi-Class Classification
>> What Is a Policy in Reinforcement Learning?
>> What Is the Difference Between Gradient Descent and Gradient Ascent?
>> Normalizing Inputs for an Artificial Neural Network
>> Introduction to Convolutional Neural Networks
>> Bias in Neural Networks
>> Understanding Dimensions in CNNs