# 深度学习

深度学习是一种利用神经网络进行机器学习的技术。了解训练、优化和使用多层神经网络的各种技术。

>> [饱和非线性](saturating-non-linearities-zh.md)
>> 人工智能图像生成器如何工作？
>> 利用LSTM防止梯度消失问题
>> [机器学习与深度学习](machine-learning-vs-deep-learning-zh.md)
>> 图神经网络简介
>> 深度伪造简介
>> 强化学习中的确定性策略与随机策略
>> [大型语言模型简介](large-language-models-zh.md)
>> Softmax 中的温度是什么，为什么要使用温度？
>> 什么是不可训练参数？
>> Epoch or Episode： 理解深度强化学习中的术语
>> [Q-Learning vs. Deep Q-Learning vs. Deep Q-Network](../ml/q-learning-vs-deep-q-learning-vs-deep-q-network-zh.md)
>> 什么是端到端深度学习？
>> 连体网络如何在图像识别中发挥作用？
>> 深度神经网络：填充
图像处理
>> 单次检测器（SSD）
>> 什么是神经网络中的 Maxout？
>> 循环神经网络
>> 神经网络中的嵌入层是什么？
>> 为什么使用替代损失？
>> 生成对抗网络：判别器损失和生成器损失
>> 神经网络中的主干是什么意思？
>> 卷积网络中的通道是什么？
>> 偏差与误差的区别
>> 实例分割与语义分割
>> 在线学习与离线学习
>> 数据扩充
>> 随机样本共识解释
概率与统计
>> 预训练神经网络意味着什么？
>> 神经网络：什么是权重衰减损失？
>> 神经网络：Conv 层和 FC 层的区别
>> 究竟什么是N符？
>> 多层感知器vs.深度神经网络
>> 无模型强化学习与基于模型的强化学习
>> 0-1损失函数解释
>> 反向传播与前馈网络的区别
>> 交叉验证：交叉验证：K-折叠 vs. 一票否决
>> 非策略与策略强化学习
>> 神经网络反向传播中的偏差更新
>> 自然语言处理中的递归与递归神经网络
>> 什么是神经网络中的 "瓶颈"？
>> 卷积神经网络与普通神经网络
>> 物体检测中的平均精度
>> 神经网络中的隐藏层
>> [监督学习和非监督学习的真实案例](examples-supervised-unsupervised-learning-zh.md)
>> [遗传算法在现实世界中的应用](genetic-algorithms-applications-zh.md)
>> 变异自动编码器中的重参数化技巧
>> 什么是机器学习中的归纳偏差？
>> 深度学习中的潜空间
>> 自编码器详解
>> 激活函数：Sigmoid vs Tanh
>> 对比学习简介
>> 机器学习中内核背后的直觉
>> 图像比较算法
>> 图像处理： 遮挡
图像处理
>> 生成模型的应用
>> 计算卷积层的输出大小
>> 生成对抗网络简介
>> 神经网络中的线性可分离数据
>> 使用 GAN 进行数据扩充
>> 学习率与批量大小的关系
>> Word2vec 单词嵌入操作： 添加、串联还是平均单词向量？
>> 异常值检测和处理
概率与统计
>> 文本分类的特征选择和缩减
>> 为什么迷你批量比包含所有训练数据的单一"批量"更好？
>> 如何创建智能聊天机器人？
>> 如何计算 CNN 的感知场大小？
>> k 近邻和高维数据
>> 在 SVM 中使用硬边际与软边际
>> 强化学习中的值迭代与策略迭代
>> 如何将文本序列转换为向量
>> 实例归一化 vs 批量归一化
>> SVM 中精度与支持向量数量之间的权衡
>> 开源神经网络库
>> 词嵌入： CBOW vs Skip-Gram
>> 自然语言处理的编码器-解码器模型
>> 神经网络中的纪元
>> 神经网络中权重的随机初始化
>> 卷积神经网络中的批量归一化
>> 神经网络与 SVM 相比的优缺点
>> 神经网络架构： 选择隐藏层的数量和大小的标准
>> 情感分析的训练数据
>> 多类分类的 F-1 分数
>> 什么是强化学习中的策略？
>> 梯度下降和梯度上升有什么区别？
>> 人工神经网络输入归一化
>> 卷积神经网络简介
>> 神经网络中的偏差
>> 理解CNN中的维度

## Deep Learning

>> Saturating Non-Linearities
>> How Do AI Image Generators Work?
>> Prevent the Vanishing Gradient Problem with LSTM
>> [Machine Learning vs. Deep Learning](https://www.baeldung.com/cs/machine-learning-vs-deep-learning)
>> An Introduction to Graph Neural Networks
>> An Introduction to Deepfakes
>> Deterministic vs. Stochastic Policies in Reinforcement Learning
>> Introduction to Large Language Models
>> What Is and Why Use Temperature in Softmax?
>> What’s a Non-trainable Parameter?
>> Epoch or Episode: Understanding Terms in Deep Reinforcement Learning
>> Q-Learning vs. Deep Q-Learning vs. Deep Q-Network
>> What Is End-to-End Deep Learning?
>> How Do Siamese Networks Work in Image Recognition?
>> Deep Neural Networks: Padding
Image Processing
>> Single Shot Detectors (SSDs)
>> What Is Maxout in a Neural Network?
>> Recurrent Neural Networks
>> What Are Embedding Layers in Neural Networks?
>> Why Use a Surrogate Loss
>> Generative Adversarial Networks: Discriminator’s Loss and Generator’s Loss
>> What Does Backbone Mean in Neural Networks?
>> What Are Channels in Convolutional Networks?
>> Differences Between Bias and Error
>> Instance Segmentation vs. Semantic Segmentation
>> Online Learning vs. Offline Learning
>> Data Augmentation
>> Random Sample Consensus Explained
Probability and Statistics
>> What Does Pre-training a Neural Network Mean?
>> Neural Networks: What Is Weight Decay Loss?
>> Neural Networks: Difference Between Conv and FC Layers
>> What Exactly Is an N-Gram?
>> Multi-Layer Perceptron vs. Deep Neural Network
>> Model-free vs. Model-based Reinforcement Learning
>> 0-1 Loss Function Explained
>> Differences Between Backpropagation and Feedforward Networks
>> Cross-Validation: K-Fold vs. Leave-One-Out
>> Off-policy vs. On-policy Reinforcement Learning
>> Bias Update in Neural Network Backpropagation
>> Recurrent vs. Recursive Neural Networks in Natural Language Processing
>> What Are “Bottlenecks” in Neural Networks?
>> Convolutional Neural Network vs. Regular Neural Network
>> Mean Average Precision in Object Detection
>> Hidden Layers in a Neural Network
>> [Real-Life Examples of Supervised Learning and Unsupervised Learning](https://www.baeldung.com/cs/examples-supervised-unsupervised-learning)
>> [Real-World Uses for Genetic Algorithms](https://www.baeldung.com/cs/genetic-algorithms-applications)
>> The Reparameterization Trick in Variational Autoencoders
>> What Is Inductive Bias in Machine Learning?
>> Latent Space in Deep Learning
>> Autoencoders Explained
>> Activation Functions: Sigmoid vs Tanh
>> An Introduction to Contrastive Learning
>> Intuition Behind Kernels in Machine Learning
>> Algorithms for Image Comparison
>> Image Processing: Occlusions
Image Processing
>> Applications of Generative Models
>> Calculate the Output Size of a Convolutional Layer
>> An Introduction to Generative Adversarial Networks
>> Linearly Separable Data in Neural Networks
>> Using GANs for Data Augmentation
>> Relation Between Learning Rate and Batch Size
>> Word2vec Word Embedding Operations: Add, Concatenate or Average Word Vectors?
>> Outlier Detection and Handling
Probability and Statistics
>> Feature Selection and Reduction for Text Classification
>> Why Mini-Batch Size Is Better Than One Single “Batch” With All Training Data
>> How to Create a Smart Chatbot?
>> How to Calculate Receptive Field Size in CNN
>> k-Nearest Neighbors and High Dimensional Data
>> Using a Hard Margin vs. Soft Margin in SVM
>> Value Iteration vs. Policy Iteration in Reinforcement Learning
>> How To Convert a Text Sequence to a Vector
>> Instance vs Batch Normalization
>> Trade-offs Between Accuracy and the Number of Support Vectors in SVMs
>> Open Source Neural Network Libraries
>> Word Embeddings: CBOW vs Skip-Gram
>> Encoder-Decoder Models for Natural Language Processing
>> Epoch in Neural Networks
>> Random Initialization of Weights in a Neural Network
>> Batch Normalization in Convolutional Neural Networks
>> Advantages and Disadvantages of Neural Networks Against SVMs
>> Neural Network Architecture: Criteria for Choosing the Number and Size of Hidden Layers
>> Training Data for Sentiment Analysis
>> F-1 Score for Multi-Class Classification
>> What Is a Policy in Reinforcement Learning?
>> What Is the Difference Between Gradient Descent and Gradient Ascent?
>> Normalizing Inputs for an Artificial Neural Network
>> Introduction to Convolutional Neural Networks
>> Bias in Neural Networks
>> Understanding Dimensions in CNNs