# 深度学习

深度学习是一种利用神经网络进行机器学习的技术。
了解训练、优化和使用多层神经网络的各种技术。

- [机器学习与深度学习](machine-learning-vs-deep-learning-zh.md)
- [开源神经网络库](ml-open-source-libraries_zh.md)
- 强化学习
  - [Epoch or Episode：理解深度强化学习中的术语](epoch-vs-episode-reinforcement-learning-zh.md)
  - [什么是强化学习中的策略？](ml-policy-reinforcement-learning-zh.md)
  - [非策略与策略强化学习](off-policy-vs-on-policy-zh.md)
  - [强化学习中的值迭代与策略迭代](ml-value-iteration-vs-policy-iteration-zh.md)
- 数据
  - 数据异常
    - [随机样本共识解释](ransac-zh.md)
    - [异常值检测和处理](ml-outlier-detection-handling-zh.md)
  - [数据扩充](ml-data-augmentation_zh.md)
    - [使用GAN进行数据扩充](../../computer-vision/ml-gan-data-augmentation-zh.md)
- 神经网络
  - [卷积神经网络简介](/ai/ai-convolutional-neural-networks_zh.md)
    - [理解CNN中的维度](ml-understanding-dimensions-cnn_zh.md)
  - [卷积神经网络与普通神经网络的对比](convolutional-vs-regular-nn-zh.md)
  - [递归神经网络](recurrent-neural-networks_zh.md)
- 神经网络架构
  - [神经网络架构：选择隐藏层数量和大小的标准](neural-networks-hidden-layers-criteria-zh.md)
  - [神经网络中的偏差](neural-networks-bias-zh.md)
  - 梯度消失
    - [利用LSTM防止梯度消失问题](lstm-vanishing-gradient-prevention-zh.md)
- 激活函数
  - [Softmax 对 Log Softmax](softmax-vs-log-softmax-zh.md)
    - [Softmax中的温度是什么，为什么要使用温度？](softmax-temperature-zh.md)
  - [ReLU vs. LeakyReLU vs. PReLU](relu-vs-leakyrelu-vs-prelu-zh.md)
  - [激活函数：Sigmoid vs Tanh](sigmoid-vs-tanh-functions-zh.md)
  - [GELU解释](gelu-activation-function-zh.md)
- 算法
  - [k近邻和高维数据](k-nearest-neighbors-zh.md)
- 大型语言模型
  - [大型语言模型简介](large-language-models-zh.md)
    - [BERT与GPT-3架构的比较](./bert-vs-gpt-3-architecture_zh.md)
  - [ChatGPT如何工作？](../../chatgpt-model_zh.md)
  - [为什么ChatGPT不擅长数学？](../../chatgpt-math-problems_zh.md)
  - [为什么ChatGPT不能一次性给出答案？](../../chatgpt-answer-sequential-words_zh.md)
  - [顶级大型语言模型对比分析](./top-llm-comparative-analysis_zh.md)
  - 变换器架构
    - [关注与自我关注](./attention-self-badhanau-differences_zh.md)
- 多模态模型
  - [谷歌DeepMind双子座简介](./gemini-google-deepmind_zh.md)
- 应用
  - [神经网络中的主干是什么意思？](neural-network-backbone-zh.md)
  - 训练
    - [预训练神经网络意味着什么？](neural-network-pre-training-zh.md)
    - [什么是不可训练参数？](non-trainable-parameter-zh.md)

- [什么是噪声对比估计损失？](pic/noise-contrastive-estimation-loss-zh.md)
- [饱和非线性](saturating-non-linearities-zh.md)
- [人工智能图像生成器如何工作？](ai-image-generation-gans-dalle-zh.md)
- [图神经网络简介](ml-gnn-zh.md)
- [深度伪造简介](deepfakes-ai-zh.md)
- [强化学习中的确定性策略与随机策略](rl-deterministic-vs-stochastic-policies-zh.md)
- [Q-Learning vs. Deep Q-Learning vs. Deep Q-Network](../../ml/q-learning-vs-deep-q-learning-vs-deep-q-network-zh.md)
- [什么是端到端深度学习？](end-to-end-deep-learning-zh.md)
- [连体网络如何在图像识别中发挥作用？](siamese-networks-zh.md)
- [深度神经网络：填充](deep-neural-networks-padding-zh.md)
- [单次检测器（SSD）](../../computer-vision/ssd-zh.md)
- [什么是神经网络中的Maxout？](maxout-neural-networks-zh.md)
- [神经网络中的嵌入层是什么？](neural-nets-embedding-layers-zh.md)
- [为什么使用替代损失？](surrogate-loss-zh.md)
- [生成对抗网络：判别器损失和生成器损失](gan-loss-zh.md)
- [卷积网络中的通道是什么？](cnn-channels-zh.md)
- [偏差与误差的区别](bias-vs-error-zh.md)
- [实例分割与语义分割](../../computer-vision/instance-semantic-segmentation-cnn-zh.md)
- [在线学习与离线学习](online-vs-offline-learning-zh.md)
- [神经网络：什么是权重衰减损失？](neural-networks-weight-decay-loss-zh.md)
- [神经网络：Conv层和FC层的区别](neural-networks-conv-fc-layers-zh.md)
- [究竟什么是N-Gram？](n-gram-zh.md)
- [多层感知器vs.深度神经网络](mlp-vs-dnn-zh.md)
- [无模型强化学习与基于模型的强化学习](ai-model-free-vs-model-based-zh.md)
- [0-1损失函数解释](ai-0-1-loss-function-zh.md)
- [反向传播与前馈网络的区别](neural-networks-backprop-vs-feedforward-zh.md)
- [交叉验证：K-Fold vs. 一票否决](cross-validation-k-fold-loo-zh.md)
- [神经网络反向传播中的偏差更新](deep-learning-bias-backpropagation-zh.md)
- [自然语言处理中的递归与递归神经网络](networks-in-nlp-zh.md)
- [什么是神经网络中的"瓶颈"？](neural-network-bottleneck-zh.md)
- [物体检测中的平均精度](../../computer-vision/ml-map-object-detection-zh.md)
- [神经网络中的隐藏层](hidden-layers-neural-network-zh.md)
- [监督学习和非监督学习的真实案例](examples-supervised-unsupervised-learning-zh.md)
- [遗传算法在现实世界中的应用](genetic-algorithms-applications-zh.md)
- [变异自动编码器中的重参数化技巧](../../computer-vision/vae-reparameterization-zh.md)
- [什么是机器学习中的归纳偏差？](ml-inductive-bias-zh.md)
- [深度学习中的潜空间](dl-latent-space-zh.md)
- [自编码器详解](autoencoders-explained-zh.md)
- [对比学习简介](contrastive-learning-zh.md)
- [机器学习中内核背后的直觉](intuition-behind-kernels-in-machine-learning-zh.md)
- [图像比较算法](image-comparison-algorithm-zh.md)
- [图像处理：遮挡](image-processing-occlusions-zh.md)
- [生成模型的应用](../../computer-vision/applications-of-generative-models-zh.md)
- [计算卷积层的输出大小](../../computer-vision/convolutional-layer-size-zh.md)
- [神经网络中的线性可分离数据](nn-linearly-separable-data-zh.md)
- [学习率与批量大小的关系](learning-rate-batch-size-zh.md)
- [Word2vec单词嵌入操作：添加、串联还是平均单词向量？](word2vec-word-embeddings-zh.md)
- [文本分类的特征选择和缩减](feature-selection-reduction-for-text-classification-zh.md)
- [为什么迷你批量比包含所有训练数据的单一"批量"更好？](mini-batch-vs-single-batch-training-data-zh.md)
- [如何创建智能聊天机器人？](smart-chatbots-zh.md)
- [如何计算CNN的感知场大小？](cnn-receptive-field-size-zh.md)
- [在SVM中使用硬边际与软边际](svm-hard-margin-vs-soft-margin-zh.md)

- [如何将文本序列转换为向量](text-sequence-to-vector-zh.md)
- [实例归一化vs批量归一化](instance-vs-batch-normalization-zh.md)
- [SVM中精度与支持向量数量之间的权衡](ml-accuracy-vs-number-of-support-vectors-svm-zh.md)
- [词嵌入：CBOWvsSkip-Gram](word-embeddings-cbow-vs-skip-gram-zh.md)
- [自然语言处理的编码器-解码器模型](nlp-encoder-decoder-models_zh.md)
- [神经网络中的纪元](epoch-neural-networks-zh.md)
- [神经网络中权重的随机初始化](ml-neural-network-weights-zh.md)
- [卷积神经网络中的批量归一化](batch-normalization-cnn_zh.md)
- [神经网络与SVM相比的优缺点](ml-ann-vs-svm-zh.md)
- [情感分析的训练数据](sentiment-analysis-training-data-zh.md)
- [多类分类的F-1分数](multi-class-f1-score-zh.md)

- [梯度下降和梯度上升有什么区别？](gradient-descent-vs-ascent-zh.md)
- [人工神经网络输入归一化](normalizing-inputs-artificial-neural-network_zh.md)
- [生成对抗网络简介](generative-adversarial-networks_zh.md)
- [PyTorch 中的 GAN 实现](./pytorch-generative-adversarial-networks_zh.md)
- [Python 中的填充](./padding-deep-learning-python_zh.md)

## [Deep Learning](https://www.baeldung.com/cs/category/ai/deep-learning)

- An Introduction to Gemini by Google DeepMind
- Comparative Analysis of Top Large Language Models
- Attention vs. Self-Attention
- GAN Implementation in PyTorch
- Padding in Python
- What Is Group Normalization?
- Comparison Between BERT and GPT-3 Architectures
- How to Use Model Temperature of GPT?
- Introduction to Convolutional Neural Networks
- Softmax vs. Log Softmax
- [GELU Explained](https://www.baeldung.com/cs/gelu-activation-function)
- [What Is Noise Contrastive Estimation Loss?](https://www.baeldung.com/cs/noise-contrastive-estimation-loss)
- [ReLU vs. LeakyReLU vs. PReLU](https://www.baeldung.com/cs/relu-vs-leakyrelu-vs-prelu)
- [Saturating Non-Linearities](https://www.baeldung.com/cs/saturating-non-linearities)
- [How Do AI Image Generators Work?](https://www.baeldung.com/cs/ai-image-generation-gans-dalle)
- Prevent the Vanishing Gradient Problem with LSTM
- Machine Learning vs. Deep Learning
- [An Introduction to Graph Neural Networks](https://www.baeldung.com/cs/ml-gnn)
- [An Introduction to Deepfakes](https://www.baeldung.com/cs/deepfakes-ai)
- [Deterministic vs. Stochastic Policies in Reinforcement Learning](https://www.baeldung.com/cs/rl-deterministic-vs-stochastic-policies)
- Introduction to Large Language Models
- What Is and Why Use Temperature in Softmax?
- What’s a Non-trainable Parameter?
- Epoch or Episode: Understanding Terms in Deep Reinforcement Learning
- [Q-Learning vs. Deep Q-Learning vs. Deep Q-Network](https://www.baeldung.com/cs/q-learning-vs-deep-q-learning-vs-deep-q-network)
- [What Is End-to-End Deep Learning?](https://www.baeldung.com/cs/end-to-end-deep-learning)
- [How Do Siamese Networks Work in Image Recognition?](https://www.baeldung.com/cs/siamese-networks)
- [Deep Neural Networks: Padding](https://www.baeldung.com/cs/deep-neural-networks-padding)
- [Single Shot Detectors (SSDs)](https://www.baeldung.com/cs/ssd)
- [What Is Maxout in a Neural Network?](https://www.baeldung.com/cs/maxout-neural-networks)
- Recurrent Neural Networks
- [What Are Embedding Layers in Neural Networks?](https://www.baeldung.com/cs/neural-nets-embedding-layers)
- [Why Use a Surrogate Loss](https://www.baeldung.com/cs/surrogate-loss)
- [Generative Adversarial Networks: Discriminator’s Loss and Generator’s Loss](https://www.baeldung.com/cs/gan-loss)
- What Does Backbone Mean in Neural Networks?
- [What Are Channels in Convolutional Networks?](https://www.baeldung.com/cs/cnn-channels)
- [Differences Between Bias and Error](https://www.baeldung.com/cs/bias-vs-error)
- Instance Segmentation vs. Semantic Segmentation
- [Online Learning vs. Offline Learning](https://www.baeldung.com/cs/online-vs-offline-learning)
- Data Augmentation
- Random Sample Consensus Explained
- What Does Pre-training a Neural Network Mean?
- [Neural Networks: What Is Weight Decay Loss?](https://www.baeldung.com/cs/neural-networks-weight-decay-loss)
- [Neural Networks: Difference Between Conv and FC Layers](https://www.baeldung.com/cs/neural-networks-conv-fc-layers)
- [What Exactly Is an N-Gram?](https://www.baeldung.com/cs/n-gram)
- [Multi-Layer Perceptron vs. Deep Neural Network](https://www.baeldung.com/cs/mlp-vs-dnn)
- [Model-free vs. Model-based Reinforcement Learning](https://www.baeldung.com/cs/ai-model-free-vs-model-based)
- [0-1 Loss Function Explained](https://www.baeldung.com/cs/ai-0-1-loss-function)
- [Differences Between Backpropagation and Feedforward Networks](https://www.baeldung.com/cs/neural-networks-backprop-vs-feedforward)
- [Cross-Validation: K-Fold vs. Leave-One-Out](https://www.baeldung.com/cs/cross-validation-k-fold-loo)
- Off-policy vs. On-policy Reinforcement Learning
- [Bias Update in Neural Network Backpropagation](https://www.baeldung.com/cs/deep-learning-bias-backpropagation)
- [Recurrent vs. Recursive Neural Networks in Natural Language Processing](https://www.baeldung.com/cs/networks-in-nlp)
- [What Are “Bottlenecks” in Neural Networks?](https://www.baeldung.com/cs/neural-network-bottleneck)
- Convolutional Neural Network vs. Regular Neural Network
- [Mean Average Precision in Object Detection](../../computer-vision/README-zh.md)
- [Hidden Layers in a Neural Network](https://www.baeldung.com/cs/hidden-layers-neural-network)
- [Real-Life Examples of Supervised Learning and Unsupervised Learning](https://www.baeldung.com/cs/examples-supervised-unsupervised-learning)
- [Real-World Uses for Genetic Algorithms](https://www.baeldung.com/cs/genetic-algorithms-applications)
- [The Reparameterization Trick in Variational Autoencoders](../../computer-vision/README-zh.md)
- [What Is Inductive Bias in Machine Learning?](https://www.baeldung.com/cs/ml-inductive-bias)
- [Latent Space in Deep Learning](https://www.baeldung.com/cs/dl-latent-space)
- [Autoencoders Explained](https://www.baeldung.com/cs/autoencoders-explained)
- [Activation Functions: Sigmoid vs Tanh](https://www.baeldung.com/cs/sigmoid-vs-tanh-functions)
- [An Introduction to Contrastive Learning](https://www.baeldung.com/cs/contrastive-learning)
- [Intuition Behind Kernels in Machine Learning](https://www.baeldung.com/cs/intuition-behind-kernels-in-machine-learning)
- [Algorithms for Image Comparison](https://www.baeldung.com/cs/image-comparison-algorithm)
- [Image Processing: Occlusions](https://www.baeldung.com/cs/image-processing-occlusions)
- [Applications of Generative Models](../../computer-vision/README-zh.md)
- [Calculate the Output Size of a Convolutional Layer](../../computer-vision/README-zh.md)
- [An Introduction to Generative Adversarial Networks](https://www.baeldung.com/cs/generative-adversarial-networks)
- [Linearly Separable Data in Neural Networks](https://www.baeldung.com/cs/nn-linearly-separable-data)
- [Using GANs for Data Augmentation](../../computer-vision/README-zh.md)
- [Relation Between Learning Rate and Batch Size](https://www.baeldung.com/cs/learning-rate-batch-size)
- [Word2vec Word Embedding Operations: Add, Concatenate or Average Word Vectors?](https://www.baeldung.com/cs/word2vec-word-embeddings)
- Outlier Detection and Handling
- [Feature Selection and Reduction for Text Classification](https://www.baeldung.com/cs/feature-selection-reduction-for-text-classification)
- [Why Mini-Batch Size Is Better Than One Single “Batch” With All Training Data](https://www.baeldung.com/cs/mini-batch-vs-single-batch-training-data)
- [How to Create a Smart Chatbot?](https://www.baeldung.com/cs/smart-chatbots)
- [How to Calculate Receptive Field Size in CNN](https://www.baeldung.com/cs/cnn-receptive-field-size)
- k-Nearest Neighbors and High Dimensional Data
- [Using a Hard Margin vs. Soft Margin in SVM](https://www.baeldung.com/cs/svm-hard-margin-vs-soft-margin)
- Value Iteration vs. Policy Iteration in Reinforcement Learning
- [How To Convert a Text Sequence to a Vector](https://www.baeldung.com/cs/text-sequence-to-vector)
- Instance vs Batch Normalization
- [Trade-offs Between Accuracy and the Number of Support Vectors in SVMs](https://www.baeldung.com/cs/ml-accuracy-vs-number-of-support-vectors-svm)
- [Open Source Neural Network Libraries](https://www.baeldung.com/cs/ml-open-source-libraries)
- [Word Embeddings: CBOW vs Skip-Gram](https://www.baeldung.com/cs/word-embeddings-cbow-vs-skip-gram)
- Encoder-Decoder Models for Natural Language Processing
- [Epoch in Neural Networks](https://www.baeldung.com/cs/epoch-neural-networks)
- [Random Initialization of Weights in a Neural Network](https://www.baeldung.com/cs/ml-neural-network-weights)
- Batch Normalization in Convolutional Neural Networks
- [Advantages and Disadvantages of Neural Networks Against SVMs](https://www.baeldung.com/cs/ml-ann-vs-svm)
- Neural Network Architecture: Criteria for Choosing the Number and Size of Hidden Layers
- [Training Data for Sentiment Analysis](https://www.baeldung.com/cs/sentiment-analysis-training-data)
- [F-1 Score for Multi-Class Classification](https://www.baeldung.com/cs/multi-class-f1-score)
- What Is a Policy in Reinforcement Learning?
- [What Is the Difference Between Gradient Descent and Gradient Ascent?](https://www.baeldung.com/cs/gradient-descent-vs-ascent)
- Normalizing Inputs for an Artificial Neural Network
- Bias in Neural Networks
- Understanding Dimensions in CNNs
