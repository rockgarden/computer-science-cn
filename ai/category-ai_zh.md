# 人工智能

探索现代人工智能的基础概念和算法，深入了解游戏引擎、手写识别和机器翻译等技术的产生思想。

神经网络 (114)Training (76)Natural Language Processing (38)Probability and Statistics (24)Image Processing (17)Reinforcement Learning (17)SVM (16)Convolutional Neural Networks (16)Regression (11)Object Detection (11)Generative 逆向网络 (9)决策树 (8)测试 (8)Python (7)几何 (7)注意力 (7)聚类 (7)优化 (7)PCA (6)随机森林 (6)元搜索 (6)矩阵 (5)奈维贝叶斯 (4)word2vec (4)数据库 (3)马尔可夫链 (3)交叉验证 (3)字符串 (3)算法 (3)算法 (3)算法 (3) 验证 (3)字符串 (3)项目管理 (2)游戏算法 (2)流行 (2)熵 (2)

- [认知计算与人工智能](./cognitive-computing-vs-ai_zh.md)
- [学习机器学习的先决条件是什么？](machine-learning-how-to-start_zh.md)

>> 计算机视觉与图像处理的区别
>> 物体识别任务及其区别

- [变换器模型中的注意机制](./attention-mechanism-transformers_zh.md)

>> 交叉验证：K-折叠与一一排除
>> 马尔科夫链与隐马尔科夫模型有何不同？
>> 决策树与随机森林
>> 生成对抗网络简介
>> 分类与聚类的区别
>> 支持向量机 (SVM)
>> 卷积神经网络简介
>> 线性回归与逻辑回归
>> Naive Bayes 分类法简解

- [PCA：主成分分析](./principal-component-analysis_zh.md)

>> 监督、半监督、无监督和强化学习简介
>> 深度学习中的训练和验证损失
>> 使用支持向量机进行多类分类
>> 机器学习： 主动学习
>> 什么是自回归模型？
>> ER 与 EER 图
>> 什么是噪声对比估计损失？
>> 什么是工业 4.0？
>> 训练过程中出现 NaNs 的常见原因
>> 数据库中的横向和纵向分区
>> 什么是方差比例？
>> 什么是高光谱成像？
>> 运动场和光流
>> 注意与自我注意
>> 在 TensorFlow 中实现 Maxout
>> PyTorch 中的 GAN 实现
>> 我们应该在高维度中使用欧氏度量吗？
>> 如何从扫描图像中检测文本块？
>> 处理随机森林中的过拟合问题
>> 什么是数字差分分析仪（DDA）？
>> 学习率预热意味着什么？
>> Python 中的填充
>> 什么是组归一化？
>> 什么是检索增强生成（RAG）？
>> 如何进行线性判别分析？
>> 如何使用 GPT 的模型温度？
>> 如何为 SVM 选择核类型？
>> Focal Loss 简介
>> 什么是图像拼接？
>> 什么是强化学习中的贝尔曼算子？
>> 径向基函数
>> 如何选择皮尔逊相关性和斯皮尔曼相关性？

- Softmax vs. Log Softmax

>> GELU 解释
>> 什么是噪声对比估计损失？
>> ReLU vs. LeakyReLU vs. PReLU
>> 如何计算分类器的 VC 维度？
>> 如何处理逻辑回归中的缺失数据？
>> 如何用 SMOTE 处理不平衡数据？
>> 如何绘制逻辑回归的决策边界？
>> 交叉熵和 KL Divergence 有什么区别？
>> 饱和非线性因素
>> 潜在空间和嵌入空间
>> 什么是 Hopfield 网络？
>> 神经网络： 阶跃卷积
>> 与标准 PCA 相比，核 PCA 有哪些优势？
>> 人工智能图像生成器如何工作？
>> 什么是 TinyML？
>> 置信区间
>> 用于聚类的 GMM
>> 什么是捆绑调整？
>> 学生 t 分布简介
>> 利用隐马尔可夫模型进行语音部分标记
>> 数据质量解释
>> 利用 LSTM 防止梯度消失问题
>> 可解释人工智能的力量和前景
>> 统计学在机器学习中的重要性
>> 如何提前停止？
>> 机器学习与深度学习
>> 图神经网络简介
>> 高斯混合模型
>> 机器学习的 Python
>> 人工智能伦理简介
>> 深度伪造简介
>> 什么是奇异值分解？
>> 机器学习中的 Bagging、Boosting 和 Stacking
>> 尖峰神经网络简介
>> 焦距和相机固有参数
>> K-Means 算法的缺点
>> 从 RNN 到变换器
>> 什么是数据科学？
>> 机器学习：分析学习
>> 统计学：相关与回归
>> 支持向量机中的 C 参数
>> NLP vs. NLU vs. NLG
>> 了解文本挖掘
>> 人工智能代理解析
>> 人脸识别如何工作？
>> 软件工程 vs. 数据科学
>> 强化学习中的确定性策略与随机策略
>> 大型语言模型简介
>> Softmax 中的温度是什么，为什么要使用温度？
>> 自组织图是如何工作的？
>> 光流：卢卡斯-卡纳德方法
>> 时间序列中的在线离群点检测算法
>> 什么是梯度方向和梯度幅度？
>> 如何分析损失图与历时图？
>> 神经网络中的密集和稀疏概念
>> 什么是计算机视觉中的等值线？
>> 什么是不可训练参数？
>> 卷积神经网络中特征图的目的是什么？
>> 什么是人机整合？
>> 神经网络如何识别图像？
>> 作为矩阵-矩阵乘法的二维卷积
>> 懒惰学习与急切学习
>> 数据挖掘系统的分类
>> 人工免疫系统如何
>> DBSCAN集群： 它是如何工作的？
>> 时代还是事件？理解深度强化学习中的术语
>> Q 学习 vs. 深度 Q 学习 vs. 深度 Q 网络
>> 地标检测简介
>> 什么是下游任务？
>> 图像识别： 一次性学习
>> 什么是神经风格转移？
>> 自动化机器学习详解
>> 图像处理： 基于图的分割
>> 什么是联合学习？
>> 理解大津的图像分割方法
>> 学习率预热是什么意思？
>> 什么是专家系统？
>> 什么是端到端深度学习？
>> 自然语言处理： Bleu Score
>> 三重损失简介
>> 图像识别中的连体网络如何工作？
>> 神经网络： 汇集层
>> 特征面如何工作？
>> 词嵌入的维度
>> 什么是信用分配问题？
>> ADAM 优化器
>> PAC 学习理论的真正含义是什么？
>> 图像处理： 采样和量化
>> 计算机视觉： 确定视频中物体的距离
>> 共现矩阵及其在 NLP 中的应用
>> 灵敏度和特异性
>> 硬投票分类器与软投票分类器
>> 深度神经网络： 填充
>> 基本矩阵与基本矩阵的区别
>> 单次检测器（SSD）
>> 什么是状态空间搜索？
>> 强化学习与最优控制的区别
>> 什么是独立成分分析（ICA）？
>> 参数模型和非参数模型的区别
>> 什么是神经网络中的 Maxout？

- 最大似然估计

>> 递归神经网络
>> 图注意网络
>> 稀疏编码神经网络
>> Luong 注意和 Bahdanau 注意的区别
>> 人工蜂群
>> 神经网络中的嵌入层是什么？
>> 铰链损失和逻辑损失的区别
>> 机器学习： 如何为训练格式化图像
>> 计算机视觉： 低级特征和高级特征的区别
>> 机器学习： 灵活和不灵活的模型
>> 什么是空间雕刻？
>> 图像生成的 VAE 与 GAN
>> AR、VR、MR 和 XR 之间的区别
>> 基于目标的代理与基于效用的代理的区别
>> 什么是受限玻尔兹曼机？
>> 什么是数据湖？
>> 什么是多任务学习？
>> 吉布斯采样简介
>> 一热编码解释
>> 启动网络简介
>> 机器学习中的特征选择
>> 计算机视觉中的翻译不变性和等差数列
>> 神经网络和深度信念网络
>> 迁移学习与元学习的区别
>> 残差网络
>> 为什么使用替代损失
>> 光流简介
>> 生成对抗网络： 判别损失和生成损失
>> 快速 R-CNN： ROI 层的目的是什么？

- 参数与超参数

>> 什么是蜂群智能？
>> 什么是无免费午餐定理？
>> 粒子群优化是如何工作的？
>> 计算机视觉简介
>> 图像中的模糊是如何产生的？
>> 神经网络中的骨干是什么意思？
>> Viola-Jones 算法
>> 姿势估计如何工作？
>> 空间金字塔池化
>> 物体检测： SSD 与 YOLO

- [了解激活函数](./activation-functions-neural-nets_zh.md)
- 激活函数： Sigmoid 与 Tanh

>> 什么是基于内容的图像检索？
>> 同步定位和绘图
>> 人工智能如何下棋？
>> 卷积网络中的通道是什么？
>> 什么是图像处理中的特征描述符？
>> 偏差与误差的区别
>> 光学字符识别如何工作？
>> 计算机视觉： 立体 3D 视觉
>> 什么是图像直方图？
>> 计算机视觉： 常用数据集
>> 实例分割与语义分割
>> 如何处理大图像以训练 CNN？
>> 什么是图像处理中的 "能量"？
>> 在线学习与离线学习
>> 什么是一类 SVM 及其工作原理？
>> 什么是机器学习中的特征重要性？
>> 数据扩充

- 什么是神经网络中的微调？

>> 随机森林与极随机树
>> 如何使用 Gabor 滤波器生成机器学习特征？
>> 随机样本共识解释
>> 预训练神经网络意味着什么？
>> 神经网络：什么是权重衰减损失？
>> 用威胁空间搜索赢得五子棋
>> 神经网络：Conv 层和 FC 层的区别
>> 神经网络：二进制输入 vs. 离散输入 vs. 连续输入
>> 梯度、随机和迷你批量梯度下降之间的区别
>> 规模不变特征变换
>> 什么是调节器？
>> 究竟什么是 N-格兰？
>> 隐马尔可夫模型与条件随机场
>> 维度的诅咒
>> 多层感知器 vs. 深度神经网络
>> 机器学习： 什么是消融研究？
>> 剪影图
>> F-Beta 分数
>> 什么是 YOLO 算法？
>> 决策树中的节点杂质
>> 无模型与基于模型的强化学习
>> 0-1 损失函数解释
>> 马尔科夫链是什么？
>> Epoch、Batch 和 Mini-batch 之间的区别
>> 缺失数据和稀疏数据的区别
>> 反向传播网络和前馈网络的区别
>> 立体视觉中的差异图
>> 非策略与策略强化学习
>> 神经网络反向传播中的偏差更新
>> 手写识别算法
>> 精确度与平均精确度
>> 比较 Naïve Bayes 和 SVM 在文本分类中的应用
>> 自然语言处理中的递归与递归神经网络
>> 物体检测中的交集与联合
>> 什么是神经网络的 "瓶颈"？
>> 强人工智能与弱人工智能的区别
>> 物体检测的平均精度
>> 神经网络中的隐藏层

- [迁移学习与领域适应](./transfer-learning-vs-domain-adaptation_zh.md)

>> 监督学习和非监督学习的真实案例
>> 遗传算法在现实世界中的应用
>> 什么是卷积神经网络的深度？
>> 变式自动编码器中的重参数化技巧
>> 什么是机器学习中的归纳偏差？
>> 人工智能、机器学习、统计学和数据挖掘之间有什么区别？
>> 自我监督学习简介
>> 深度学习中的潜空间
>> 自编码器详解
>> 成本、损失和目标函数的区别

>> 对比学习简介
>> 梯度提升树 vs. 随机森林
>> 机器学习的基本概念
>> 机器学习核背后的直觉
>> 图像比较算法
>> 图像处理： 遮挡
>> 机器学习中的信息增益
>> 交叉验证和决策树
>> 如何在神经网络中使用 K 折交叉验证？
>> 生成模型的应用
>> 计算卷积层的输出大小
>> 曲线拟合简介
>> 用于分类的 K-Means

- ML：训练、验证和测试

>> 隐马尔可夫模型简介
>> Q-Learning vs. SARSA
>> SGD 与反向传播的区别
>> 神经网络中的线性可分离数据
>> 随机森林中树的深度和数量的影响
>> 使用 GAN 进行数据扩充
>> 双向和单向 LSTM 的区别
>> 机器学习中的特征、参数和类
>> 学习率与批量大小的关系
>> Word2vec 单词嵌入操作： 添加、串联还是平均单词向量？
>> 机器学习中的漂移、异常和新颖性
>> 马尔可夫决策过程： 值迭代是如何工作的？
>> 什么是选择偏差，如何防止？
>> 集合学习
>> 决策树 vs. Naive Bayes 分类器
>> 机器学习中的精度与 AUC
>> 贝叶斯网络
>> 机器学习中的偏差
>> 主题建模中的一致性得分是好是坏？
>> 马尔科夫链聊天机器人如何工作？
>> 机器学习中的分层抽样
>> 异常值检测和处理
>> WEKA 中的数据挖掘
>> 选择学习率
>> 机器学习中的欠拟合和过拟合
>> "20个问题"人工智能算法如何工作？
>> 如何计算线性回归中的正则化参数
>> 如何从分词的 Word2vec 中获取句子向量
>> Q 学习与动态编程
>> 文本分类的特征选择和缩减
>> PCA 中要取多少主成分？
>> SVM 与感知器的区别
>> 为什么迷你批量比包含所有训练数据的单一"批量"更好？

- 如何设计深度卷积神经网络？

>> 期望最大化（EM）技术的直观解释
>> 时间序列中的模式识别
>> 如何创建智能聊天机器人？
>> 开源人工智能引擎
>> NLP 的 word2vec：负采样解释
>> 随机森林中的袋外误差
>> 如何计算 CNN 的感受野大小
>> k 近邻和高维数据
>> 在 SVM 中使用硬边际 vs. 软边际

- 强化学习中的值迭代与策略迭代

>> 如何将文本序列转换为向量
>> 实例归一化 vs 批量归一化
>> SVM 中精度与支持向量数量之间的权衡

- 开放源代码神经网络库
- 变换器文本嵌入

>> 生成算法与判别算法
>> 两个短语的语义相似性
>> SVM 中为什么要进行特征缩放？
>> 自动提取关键词和关键句
>> 线性回归中的归一化与标准化
>> 词嵌入： CBOW vs Skip-Gram
>> 字符串相似性度量： 基于序列
>> 如何提高 Naive Bayes 分类性能？
>> 丑小鸭定理
>> 使用 Word2Vec 进行主题建模
>> 归一化表的特征
>> 字符串相似性度量： 标记方法
>> 逻辑回归中的梯度下降方程
>> 相关特征与分类准确性
>> 弱监督学习
>> 机器学习模型的损失和准确率的解释
>> 分割数据集
>> 用于自然语言处理的编码器-解码器模型
>> 解决 K 臂强盗问题
>> 神经网络中的纪元

- Epsilon-Greedy Q-learning

>> 神经网络中权重的随机初始化
>> 潜在德里希勒分配的主题建模
>> 字符串相似度量--编辑距离
>> 卷积神经网络中的批归一化
>> 将单词转换为向量
>> 神经网络的强化学习
>> 将均匀分布转换为正态分布
>> 梯度下降法与牛顿梯度下降法的比较
>> 谷歌 "你是说 "算法是如何工作的？算法是如何工作的？
>> 什么是交叉熵？
>> 确定文本情感的算法
>> 神经网络与 SVM 相比的优缺点
>> Top-N 准确度指标
>> 情感分析词典
>> 神经网络架构： 选择隐藏层的数量和大小的标准
>> 情感分析的训练数据
>> 多类分类的 F-1 分数

- [神经网络中纪元与迭代的区别](./neural-networks-epoch-vs-iteration_zh.md)

>> 什么是强化学习中的策略？
>> 梯度下降和梯度上升有何区别？
>> SVM 与神经网络
>> 列文森距离计算
>> 人工神经网络输入的归一化

- 什么是机器学习中的学习曲线？

>> 分类模型评估简介
>> 为什么逻辑回归的代价函数是对数表达式？
>> 神经网络中的偏差
>> 如何计算两篇文本文档的相似度？
>> 如何构建知识图谱？
>> 特征和标签的区别
>> 大数据与数据挖掘
>> 分割数据集之前还是之后的数据规范化？
>> 有标签和无标签数据的区别是什么？
>> 书面文本中的情感检测简介

- [神经网络中的神经元](neural-networks-neurons-zh.md)
- [神经网络的优缺点](./neural-net-advantages-disadvantages_zh.md)

>> 公开可用的垃圾邮件过滤器训练集
>> 特征缩放
>> 了解 CNN 的维度

- [ReLU 和 Dropout 层在 CNN 中的工作原理](./ml-relu-dropout-layers_zh.md)
- [反向传播神经网络中的非线性激活函数](./ml-nonlinear-activation-functions_zh.md)
- [线性模型的不足：通往非线性函数之路](./ml-linear-activation-functions_zh.md)

>> 遗传算法与神经网络
>> 计算机科学中的谓词
>> 向未知数量的聚类

[深度学习](./ml/deep-learning/category-ai-deep-learning_zh.md)

- 为什么 ChatGPT 不能一次性给出答案？
- BERT 和 GPT-3 架构的比较
- 为什么 ChatGPT 不擅长数学？
- ChatGPT 如何工作？
- 卷积神经网络与普通神经网络对比
- 谷歌 DeepMind 的 Gemini 简介

## [Artificial Intelligence](https://www.baeldung.com/cs/category/ai)

>> Differences Between Computer Vision and Image Processing
>> Object Recognition Tasks and Their Differences

- Attention Mechanism in the Transformers Model
- Cross-Validation: K-Fold vs. Leave-One-Out

>> What Is the Difference Between Markov Chains and Hidden Markov Models?
>> Decision Trees vs. Random Forests

- An Introduction to Generative Adversarial Networks

>> Differences Between Classification and Clustering
>> Support Vector Machines (SVM)
>> Introduction to Convolutional Neural Networks
>> Linear Regression vs. Logistic Regression
>> A Simple Explanation of Naive Bayes Classification

- PCA: Principal Component Analysis
- Introduction to Supervised, Semi-supervised, Unsupervised and Reinforcement Learning

>> Training and Validation Loss in Deep Learning

- Multiclass Classification Using Support Vector Machines

>> Machine Learning: Active Learning
>> What Is An Autoregressive Model?
>> ER vs. EER Diagrams
>> What Is Noise Contrastive Estimation Loss?

- An Introduction to Gemini by Google DeepMind

>> What Is Industry 4.0?
>> Common Causes of NaNs During Training
>> Horizontal and Vertical Partitioning in Databases
>> What Is Proportion of Variance?
>> What Is Hyperspectral Imaging?
>> Motion Field and Optical Flow
>> Attention vs. Self-Attention
>> Maxout Implementation in TensorFlow

- GAN Implementation in PyTorch

>> Should We Use the Euclidean Metric in High Dimensions?
>> How Can We Detect Blocks of Text From Scanned Images?
>> Dealing with Overfitting in Random Forests
>> What is Digital Differential Analyzer (DDA)?
>> What Does Learning Rate Warm-up Mean?

- Padding in Python
- What Is Group Normalization?

>> What Is Retrieval-Augmented Generation (RAG)?
>> How to Perform Linear Discriminant Analysis?

- Comparison Between BERT and GPT-3 Architectures
- How to Use Model Temperature of GPT?

>> How to Select the Type of Kernel for a SVM?
>> An Introduction to Focal Loss
>> What Is Image Stitching?
>> What Is the Bellman Operator in Reinforcement Learning?
>> Radial Basis Function
>> How to Choose Between Pearson and Spearman Correlation?

- Softmax vs. Log Softmax

>> GELU Explained
>> What Is Noise Contrastive Estimation Loss?
>> ReLU vs. LeakyReLU vs. PReLU
>> How to Calculate the VC-Dimension of a Classifier?

- Why Does ChatGPT Not Give the Answer All at Once?

>> How to Handle Missing Data in Logistic Regression?
>> How to Handle Unbalanced Data With SMOTE?
>> How to Plot Logistic Regression’s Decision Boundary?
>> What’s the Difference Between Cross-Entropy and KL Divergence?
>> Saturating Non-Linearities
>> Latent and Embedding Space
>> What Is A Hopfield Network?
>> Neural Networks: Strided Convolutions
>> What Are the Advantages of Kernel PCA Over Standard PCA?
>> How Do AI Image Generators Work?
>> What Is TinyML?
>> Confidence Intervals
>> GMMs for Clustering
>> What Is Bundle Adjustment?
>> Introduction to the Student’s t-distribution
>> Part-of-Speech Tagging With Hidden Markov Model
>> Data Quality Explained
>> Prevent the Vanishing Gradient Problem with LSTM
>> The Power and Promise of Explainable AI
>> Importance of Statistics in Machine Learning
>> How to Do Early Stopping?
>> Machine Learning vs. Deep Learning
>> An Introduction to Graph Neural Networks
>> Gaussian Mixture Models
>> Python for Machine Learning

- Why Is ChatGPT Bad at Math?

>> Introduction to AI Ethics
>> An Introduction to Deepfakes
>> What Is Singular Value Decomposition?
>> Bagging, Boosting, and Stacking in Machine Learning
>> Introduction to Spiking Neural Networks
>> Focal Length and Intrinsic Camera Parameters
>> The Drawbacks of K-Means Algorithm

- From RNNs to Transformers

>> What Is Data Science?

- How Does ChatGPT Work?

>> Machine Learning: Analytical Learning
>> Statistics: Correlation vs. Regression
>> The C Parameter in Support Vector Machines
>> NLP vs. NLU vs. NLG
>> Understanding Text Mining
>> Artificial Intelligence Agents Explained
>> How Does Face Recognition Work?
>> Software Engineering vs. Data Science
>> Deterministic vs. Stochastic Policies in Reinforcement Learning

- Introduction to Large Language Models
- What Is and Why Use Temperature in Softmax?

>> How Do Self-Organizing Maps Work?
>> Optical Flow: Lucas-Kanade Method
>> Algorithm for Online Outlier Detection in Time Series
>> What Is Gradient Orientation and Gradient Magnitude?
>> How to Analyze Loss vs. Epoch Graphs?
>> The Concepts of Dense and Sparse in the Context of Neural Networks
>> What Are Contours in Computer Vision?
>> What’s a Non-trainable Parameter?
>> What Is the Purpose of a Feature Map in a Convolutional Neural Network
>> What Is Human-Machine Integration?
>> How Does a Neural Network Recognize Images?
>> 2D Convolution as a Matrix-Matrix Multiplication
>> Lazy vs. Eager Learning
>> Classification of Data Mining Systems
>> How Do Artificial Immune Systems Work?
>> DBSCAN Clustering: How Does It Work?

- Epoch or Episode: Understanding Terms in Deep Reinforcement Learning

>> Q-Learning vs. Deep Q-Learning vs. Deep Q-Network
>> Introduction to Landmark Detection
>> What Are Downstream Tasks?
>> Image Recognition: One-Shot Learning
>> What Is Neural Style Transfer?
>> Automated Machine Learning Explained
>> Image Processing: Graph-based Segmentation
>> What Is Federated Learning?
>> Understanding Otsu’s Method for Image Segmentation
>> What Does Learning Rate Warm-up Mean?
>> What Are Expert Systems?
>> What Is End-to-End Deep Learning?
>> Natural Language Processing: Bleu Score
>> Introduction to Triplet Loss
>> How Do Siamese Networks Work in Image Recognition?
>> Neural Networks: Pooling Layers
>> How Do Eigenfaces Work?
>> Dimensionality of Word Embeddings
>> What Is the Credit Assignment Problem?
>> ADAM Optimizer
>> What Does PAC Learning Theory Really Mean?
>> Image Processing: Sampling and Quantization
>> Computer Vision: Determining the Distance From an Object in a Video
>> Co-occurrence Matrices and Their Uses in NLP
>> Sensitivity and Specificity
>> Hard vs. Soft Voting Classifiers
>> Deep Neural Networks: Padding
>> Difference Between Fundamental Matrix and Essential Matrix
>> Single Shot Detectors (SSDs)
>> What Is State Space Search?
>> Difference Between Reinforcement Learning and Optimal Control
>> What Is Independent Component Analysis (ICA)?
>> Differences Between a Parametric and Non-parametric Model
>> What Is Maxout in a Neural Network?

- Maximum Likelihood Estimation

- Recurrent Neural Networks

>> Graph Attention Networks
>> Sparse Coding Neural Networks
>> Differences Between Luong Attention and Bahdanau Attention
>> Artificial Bee Colony
>> What Are Embedding Layers in Neural Networks?
>> Differences Between Hinge Loss and Logistic Loss
>> Machine Learning: How to Format Images for Training
>> Computer Vision: Differences Between Low-Level and High-Level Features
>> Machine Learning: Flexible and Inflexible Models
>> What Is Space Carving?
>> VAE Vs. GAN For Image Generation
>> Differences Between AR, VR, MR, and XR
>> Difference Between Goal-based and Utility-based Agents
>> What Are Restricted Boltzmann Machines?
>> What Is a Data Lake?
>> What Is Multi-Task Learning?
>> Introduction to Gibbs Sampling
>> One-Hot Encoding Explained
>> Introduction to Inception Networks
>> Feature Selection in Machine Learning

- Cognitive Computing vs. Artificial Intelligence

>> Translation Invariance and Equivariance in Computer Vision
>> Neural Network and Deep Belief Network
>> Differences Between Transfer Learning and Meta-Learning
>> Residual Networks
>> Why Use a Surrogate Loss
>> Introduction to Optical Flow
>> Generative Adversarial Networks: Discriminator’s Loss and Generator’s Loss
>> Fast R-CNN: What is the Purpose of the ROI Layers?

- Parameters vs. Hyperparameters

>> What Is Swarm Intelligence?
>> What Is the No Free Lunch Theorem?
>> How Does Particle Swarm Optimization Work?
>> An Introduction to Computer Vision
>> How Do Blurs in Images Work?
>> What Does Backbone Mean in Neural Networks?
>> The Viola-Jones Algorithm
>> How Does Pose Estimation Work?
>> Spatial Pyramid Pooling
>> Object Detection: SSD Vs. YOLO

- Understanding Activation Functions

>> What Is Content-Based Image Retrieval?
>> Simultaneous Localization and Mapping
>> How Does AI Play Chess?
>> What Are Channels in Convolutional Networks?
>> What Is a Feature Descriptor in Image Processing?
>> Differences Between Bias and Error
>> How Does Optical Character Recognition Work
>> Computer Vision: Stereo 3D Vision
>> What Are Image Histograms?
>> Computer Vision: Popular Datasets
>> Instance Segmentation vs. Semantic Segmentation
>> How to Handle Large Images to Train CNNs?
>> What Is “Energy” in Image Processing?

- Neurons in Neural Networks

>> Online Learning vs. Offline Learning
>> What Is One Class SVM and How Does It Work?

- What Is Feature Importance in Machine Learning?
- Data Augmentation
- What Is Fine-Tuning in Neural Networks?

>> Random Forest vs. Extremely Randomized Trees
>> How to Use Gabor Filters to Generate Features for Machine Learning
>> Random Sample Consensus Explained

- What Does Pre-training a Neural Network Mean?

>> Neural Networks: What Is Weight Decay Loss?
>> Win Gomoku with Threat Space Search
>> Neural Networks: Difference Between Conv and FC Layers

- Neural Networks: Binary vs. Discrete vs. Continuous Inputs

>> Differences Between Gradient, Stochastic and Mini Batch Gradient Descent
>> Scale-Invariant Feature Transform
>> What Is a Regressor?
>> What Exactly Is an N-Gram?
>> Hidden Markov Models vs. Conditional Random Fields
>> The Curse of Dimensionality
>> Multi-Layer Perceptron vs. Deep Neural Network
>> Machine Learning: What Is Ablation Study?
>> Silhouette Plots
>> F-Beta Score
>> What Is YOLO Algorithm?
>> Node Impurity in Decision Trees
>> Model-free vs. Model-based Reinforcement Learning
>> 0-1 Loss Function Explained
>> Is a Markov Chain the Same as a Finite State Machine?
>> Differences Between Epoch, Batch, and Mini-batch
>> Differences Between Missing Data and Sparse Data
>> Differences Between Backpropagation and Feedforward Networks
>> Disparity Map in Stereo Vision

- Off-policy vs. On-policy Reinforcement Learning

>> Bias Update in Neural Network Backpropagation
>> Algorithm for Handwriting Recognition
>> Precision vs. Average Precision
>> Comparing Naïve Bayes and SVM for Text Classification
>> Recurrent vs. Recursive Neural Networks in Natural Language Processing
>> Intersection Over Union for Object Detection
>> What Are “Bottlenecks” in Neural Networks?

- Convolutional Neural Network vs. Regular Neural Network

>> Differences Between Strong-AI and Weak-AI
>> Mean Average Precision in Object Detection
>> Hidden Layers in a Neural Network

- Transfer Learning vs Domain Adaptation

>> Real-Life Examples of Supervised Learning and Unsupervised Learning
>> Real-World Uses for Genetic Algorithms
>> What Is Depth in a Convolutional Neural Network?
>> The Reparameterization Trick in Variational Autoencoders
>> What Is Inductive Bias in Machine Learning?
>> What Is the Difference Between Artificial Intelligence, Machine Learning, Statistics, and Data Mining?
>> An Introduction to Self-Supervised Learning
>> Latent Space in Deep Learning
>> Autoencoders Explained
>> Difference Between the Cost, Loss, and the Objective Function
>> Activation Functions: Sigmoid vs Tanh
>> An Introduction to Contrastive Learning
>> Gradient Boosting Trees vs. Random Forests
>> Basic Concepts of Machine Learning
>> Intuition Behind Kernels in Machine Learning
>> Algorithms for Image Comparison
>> Image Processing: Occlusions
>> Information Gain in Machine Learning
>> Cross-Validation and Decision Trees
>> How to Use K-Fold Cross-Validation in a Neural Network?
>> Applications of Generative Models
>> Calculate the Output Size of a Convolutional Layer
>> Introduction to Curve Fitting
>> K-Means for Classification

- ML: Train, Validate, and Test

>> An Introduction to the Hidden Markov Model
>> Q-Learning vs. SARSA
>> Differences Between SGD and Backpropagation
>> Linearly Separable Data in Neural Networks
>> The Effects of the Depth and Number of Trees in a Random Forest

- Using GANs for Data Augmentation
- Differences Between Bidirectional and Unidirectional LSTM
- Features, Parameters and Classes in Machine Learning

>> Relation Between Learning Rate and Batch Size
>> Word2vec Word Embedding Operations: Add, Concatenate or Average Word Vectors?
>> Drift, Anomaly, and Novelty in Machine Learning
>> Markov Decision Process: How Does Value Iteration Work?
>> What Is Selection Bias and How Can We Prevent It?
>> Ensemble Learning
>> Decision Tree vs. Naive Bayes Classifier
>> Accuracy vs AUC in Machine Learning
>> Bayesian Networks
>> Biases in Machine Learning
>> When Coherence Score Is Good or Bad in Topic Modeling?
>> How Do Markov Chain Chatbots Work?
>> Stratified Sampling in Machine Learning

- Outlier Detection and Handling

>> Data Mining in WEKA
>> Choosing a Learning Rate
>> Underfitting and Overfitting in Machine Learning
>> How Do “20 Questions” AI Algorithms Work?
>> How to Calculate the Regularization Parameter in Linear Regression
>> How to Get Vector for A Sentence From Word2vec of Tokens
>> Q-Learning vs. Dynamic Programming
>> Feature Selection and Reduction for Text Classification
>> How Many Principal Components to Take in PCA?
>> Difference Between a SVM and a Perceptron
>> Why Mini-Batch Size Is Better Than One Single “Batch” With All Training Data

- How to Design Deep Convolutional Neural Networks?

>> Intuitive Explanation of the Expectation-Maximization (EM) Technique
>> Pattern Recognition in Time Series
>> How to Create a Smart Chatbot?

- Open-Source AI Engines

>> NLP’s word2vec: Negative Sampling Explained

- Out-of-bag Error in Random Forests

>> How to Calculate Receptive Field Size in CNN

- k-Nearest Neighbors and High Dimensional Data

>> Using a Hard Margin vs. Soft Margin in SVM

- Value Iteration vs. Policy Iteration in Reinforcement Learning

>> How To Convert a Text Sequence to a Vector

- Instance vs Batch Normalization

>> Trade-offs Between Accuracy and the Number of Support Vectors in SVMs

- Open Source Neural Network Libraries
- Transformer Text Embeddings

>> Generative vs. Discriminative Algorithms
>> Semantic Similarity of Two Phrases
>> Why Feature Scaling in SVM?
>> Automatic Keyword and Keyphrase Extraction
>> Normalization vs Standardization in Linear Regression
>> Word Embeddings: CBOW vs Skip-Gram
>> String Similarity Metrics: Sequence Based
>> How to Improve Naive Bayes Classification Performance?
>> Ugly Duckling Theorem
>> Topic Modeling with Word2Vec
>> Normalize Features of a Table
>> String Similarity Metrics: Token Methods
>> Gradient Descent Equation in Logistic Regression

- Correlated Features and Classification Accuracy

>> Weakly Supervised Learning
>> Interpretation of Loss and Accuracy for a Machine Learning Model
>> Splitting a Dataset into Train and Test Sets

- Encoder-Decoder Models for Natural Language Processing

>> Solving the K-Armed Bandit Problem

- Epoch in Neural Networks

- Epsilon-Greedy Q-learning

- Random Initialization of Weights in a Neural Network

>> Topic Modeling with Latent Dirichlet Allocation
>> String Similarity Metrics – Edit Distance

- Batch Normalization in Convolutional Neural Networks

>> Converting a Word to a Vector
>> Reinforcement Learning with Neural Network
>> Converting a Uniform Distribution to a Normal Distribution
>> Gradient Descent vs. Newton’s Gradient Descent
>> How Does the Google “Did You Mean?” Algorithm Work?

- What Is Cross-Entropy?

>> Algorithms for Determining Text Sentiment
>> Advantages and Disadvantages of Neural Networks Against SVMs
>> Top-N Accuracy Metrics
>> Sentiment Analysis Dictionaries

- Neural Network Architecture: Criteria for Choosing the Number and Size of Hidden Layers

>> Training Data for Sentiment Analysis
>> F-1 Score for Multi-Class Classification

- The Difference Between Epoch and Iteration in Neural Networks
- What Is a Policy in Reinforcement Learning?
- What Is the Difference Between Gradient Descent and Gradient Ascent?

>> SVM Vs Neural Network
>> Levenshtein Distance Computation

- Normalizing Inputs for an Artificial Neural Network

- What Is a Learning Curve in Machine Learning?

>> Introduction to the Classification Model Evaluation
>> Why Does the Cost Function of Logistic Regression Have a Logarithmic Expression?
>> Bias in Neural Networks
>> How to Compute the Similarity Between Two Text Documents?
>> How to Build a Knowledge Graph?

- Difference Between a Feature and a Label

>> Big Data vs Data Mining
>> Data Normalization Before or After Splitting a Data Set?
>> What Is the Difference Between Labeled and Unlabeled Data?
>> Introduction to Emotion Detection in Written Text

- What Are the Prerequisites for Studying Machine Learning?
- Advantages and Disadvantages of Neural Networks

>> Publicly Available Spam Filter Training Sets

- Feature Scaling
- Understanding Dimensions in CNNs
- How ReLU and Dropout Layers Work in CNNs
- Nonlinear Activation Functions in a Backpropagation Neural Network
- Inadequacy of Linear Models: the Road to Nonlinear Functions

>> Genetic Algorithms vs Neural Networks
>> Predicates in Computer Science

- Clustering Into an Unknown Number of Clusters
